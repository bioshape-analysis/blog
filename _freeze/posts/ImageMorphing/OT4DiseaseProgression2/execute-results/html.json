{
  "hash": "45671807a3e48ddd0fc2f02b3fd5d9e1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Optimal Mass Transport for Shape Progression Study\"\nsubtitle: \"PyTorch Implementation of Benamou-Brenier Formulation\"\n\n\nauthor:\n  - name: \"Siddharth Rout\"\n    email: \"siddharth.rout@ubc.ca\"\n    url: \"https://scholar.google.com/citations?user=2r63M5kAAAAJ&hl=en\"\n\ndate: \"December 16 2024\"\ncategories: [optimal transport, shape morphing, Benamou-Brenier's Formulation, pytorch, automatic differentiation]\n\ncallout-icon: false\nformat:\n  html:\n    code-fold: true\nbibliography: bibliography.bib\n\nexecute:\n  echo: true\n  freeze: auto\n  warning: false\n\n---\n\n\n# Introduction\nIn this blog post, let us continue from the previous blog post on defining the convex formulation of optimal mass transport (OMT).\nSpecifically, let us look upon the need for application of OMT for interpolation of shapes and learn to implement Benamou-Brenier's formulation of OMT in python.\nThe problem is essentially a PDE optimization process and hence the numerical solution is expensive. The aglorithm used here is based on an efficient\nmethod as described in (@EldadOMT). It relies on augmented Lagrangian approach to efficiently solve the constrained optimization.\nAlso, the use of graphics processing unit (GPU) has been evident in accelerating the computation time. (@REHMANGPU)\nHence, [PyTorch](https://en.wikipedia.org/wiki/PyTorch) is used for programming the problem here, as it is an excellent\neasy to use scientific computing framework with the feature of automatic differentiation (@AD2010) accelerated by GPUs for tensor operations.\n\n# Mathematical Formulation\nThe Benamou-Brenier formulation (@benamou2000computational) is used, which interprets it as a fluid flow problem.\nThis approach finds the path of continual mass transfer such that minimal “kinetic energy” needed to transform from one state to another.\n\nThe Benamou-Brenier formulation considers a probability density $\\rho(x, t)$ evolving over time\n$t \\in [0, 1]$ from an initial distribution $\\rho_0$ to a final distribution $\\rho_1$. The goal is to\nfind a velocity field $v(x, t)$ that minimizes the action, or \"kinetic energy\" cost:\n\n$$\n\\min_{\\rho, v} \\int_0^1 \\int_X \\frac{1}{2} \\|v(x, t)\\|^2 \\rho(x, t) \\, dx \\, dt,\n$$\n\nsubject to the **continuity equation**:\n\n$$\n\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho v) = 0,\n$$\n\nwhich ensures mass conservation from $\\rho_0$ to $\\rho_1$.\n\nThe problem is reposed in terms of momentum in x and y respectively as $m_x = \\rho v_x$ and $m_y = \\rho v_y$ as in (@EldadOMT).\nStaggered grid is used for $m_x$, $m_y$, and $\\rho$ to stabilize non physical checkerboard instability.\n\nSo, the problem is rewritten as:\n\n$$\n\\min_{m} \\int_0^1 \\int_{\\Omega} \\frac{1}{\\rho} (m_x^2 + m_y^2)  \\, dx \\, dt,\n$$\n\nsuch that\n\n$$\n    \\Delta_x m_x + \\Delta_y m_y + \\Delta_t \\rho = 0,\n$$\n\nwhere $\\rho$(x,y,0) = Image at time 0, and $\\rho$(x,y,1) = Image at time 1.\n\nFormulating this constrained optimization problem into a KKT problem by [Augmented Lagrangian method](https://en.wikipedia.org/wiki/Augmented_Lagrangian_method) (@nocedal1999numerical)\nwhich is then optimized by Adam's stochastic gradient descent optimization. (@kingma2014adam)\n\n# Case Study: Need for a Special Interpolation\n\nLet us take the MRI image samples (@fig-BrainMRI) of the vertical and horizontal cross-sections a kid's brain at the age of 5 years, 7 years and 9 years,\nas shared by (@neurodegeneration2020). What if we did not have the MRI scans when the kid was 7 year old?\nSpecifically, if the kid has a disease. If we have an interpolation technique, we can use the scans at the ages 5 and 9 years and\nlook at the sequence of interpolated scans to study which part of brain is under compression and gauge how it affected over time.\n\n![State of neurodegeneration in a kid at different ages. (@neurodegeneration2020) OT can learn the\nprogression or the transformation (T) of brain deformation from the state at 5 year age ($\\rho_0$) to the final\nstate at 7 year age ($\\rho_1$) or 9 year age ($\\rho_2$).](mri_agewise_annotated.jpg){#fig-BrainMRI}\n\nThe popular interpolation linear interpolation or any polynomial interpolation is not the best choice in such a case.\nTo study the concept and learn why, let us take a toy case of aging cell. Essentially to generate the interpolation\nof the shape of a living cell from the images of a young and healthy cell to an old deformed cell.\n\n::: {#fig-cellsenescence layout-ncol=2}\n\n![A young and healthy cell.](HealthyCell.png){#fig-a width=150}\n\n![An old deformed cell.](OldCell.png){#fig-b width=150}\n\nSenescence of a toy cell\n:::\n\n# Linear Interpolation\nThe basic and most simple method of interpolation is linear interpolation. It is a commonly used technique for generating intermediate images between two given images.\nIt assumes a linear blend of pixel intensities over the interpolation parameter 't', which is time. Mathematically, the equation of the interpolant($I_t$) is given by:\n\n$$\nI_t = (1 - t) I_0 + t I_1,\n$$\nwhere $I_0$ is the image at time 0 and $I_1$ is the image at time 1.\n\n![Interpolated shape of the toy cell at 5 different intermediate stages.](AgingCellbyLinearInterpolation.png){#fig-LInt}\n\n\nThe @fig-LInt shows the interpolation of the images of the toy cell when it is healthy and when it is old.\nIt can be seen that what essentially happens is a smooth transition between the two images by linearly interpolating\nthe intensity values of each pixel independently. The transform in shape is not something we see in real life. It is not really a physical transformation.\nThe masses/pixels do not really transport in spatial domain, rather they teleport. This kind of interpolation is hence quite often not beneficial.\nIn the case of MRI scans, @fig-LIntBrain shows how the linearly interpolated shape at different ages do not make sense physically.\nLet us implement an OMT based interpolation on a to a toy problem, whose initial and final images are randomly sampled from MNIST digits, can be seen in @fig-example.\n\n![Linear interpolation of brain MRI at 5 different ages.](LinearBrain.png){#fig-LIntBrain}\n\n# PyTorch Implementation of OMT\n## Example Images\n![Images to use for OMT.](Example.png){#fig-example width=400}\n\n[Click to download the first image file.](x0.pt)\n\n[Click to download the second image file.](x1.pt)\n\n## Import Libraries\n\n::: {#eaad1712 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nimport torch\nfrom torch import nn\nfrom torchvision.io import read_image\nimport torch.nn.functional as F\n```\n:::\n\n\nThe most import library, here, is torch as it has the ability to perform automatic differentiation and GPU accelerated tensor operations.\nTorchvision is required for additional image operations and matplotlib for plots.\n\n## OMT Class\n\n::: {#68e3cbb6 .cell execution_count=2}\n``` {.python .cell-code}\nclass omtFD(nn.Module):\n    def __init__(self, n1, n2, n3, bs=1):\n        super().__init__()\n        self.n1 = n1\n        self.n2 = n2\n        self.n3 = n3\n        self.bs = bs\n\n        self.m1 = nn.Parameter(torch.zeros(bs, 1, n1 - 1, n2, n3))\n        self.m2 = nn.Parameter(torch.zeros(bs, 1, n1, n2 - 1, n3))\n        self.rho = nn.Parameter(torch.zeros(bs, 1, n1, n2, n3 - 1) - 3)\n\n    def OMTconstraintMF(self, I1, I2):\n        dx, dy, dt = 1 / self.n1, 1 / self.n2, 1 / self.n3\n\n        ox = torch.zeros(self.bs, 1, 1, self.n2, self.n3).to(self.rho.device)\n        oy = torch.zeros(self.bs, 1, self.n1, 1, self.n3).to(self.rho.device)\n\n        I1 = I1.unsqueeze(-1)\n        I2 = I2.unsqueeze(-1)\n        rhop = torch.exp(self.rho)\n\n        # Add I1 and I2 to rho\n        rhoBC = torch.cat((I1, rhop, I2), dim=-1)\n        m1BC = torch.cat((ox, self.m1, ox), dim=-3)\n        m2BC = torch.cat((oy, self.m2, oy), dim=-2)\n\n        # Compute the Div\n        m1x = (m1BC[:, :, :-1, :, :] - m1BC[:, :, 1:, :, :]) / dx\n        m2y = (m2BC[:, :, :, :-1, :] - m2BC[:, :, :, 1:, :]) / dy\n        rhot = (rhoBC[:, :, :, :, :-1] - rhoBC[:, :, :, :, 1:]) / dt\n        return m1x + m2y + rhot\n\n    def OMTgradRho(self, I1, I2):\n        I1 = I1.unsqueeze(-1)\n        I2 = I2.unsqueeze(-1)\n\n        # Add I1 and I2 to rho\n        rhop = torch.exp(self.rho)\n        rhoBC = torch.cat((I1, rhop, I2), dim=-1)\n        rhox = (rhoBC[:, :, :-1, :, :] - rhoBC[:, :, 1:, :, :])\n        rhoy = (rhoBC[:, :, :, :-1, :] - rhoBC[:, :, :, 1:, :])\n        rhot = (rhoBC[:, :, :, :, :-1] - rhoBC[:, :, :, :, 1:])\n\n        return rhox, rhoy, rhot\n\n    def OMTobjfunMF(self, I1, I2):\n        I1 = I1.unsqueeze(-1)\n        I2 = I2.unsqueeze(-1)\n\n        rhop = torch.exp(self.rho)\n\n        ox = torch.zeros(self.bs, 1, 1, self.n2, self.n3).to(self.rho.device)\n        oy = torch.zeros(self.bs, 1, self.n1, 1, self.n3).to(self.rho.device)\n\n        # Add I1 and I2 to rho\n        rhoBC = torch.cat((I1, rhop, I2), dim=-1)\n        m1BC = torch.cat((ox, self.m1, ox), dim=-3)\n        m2BC = torch.cat((oy, self.m2, oy), dim=-2)\n\n        # Average quantities to cell center\n        m1c = (m1BC[:, :, :-1, :, :] + m1BC[:, :, 1:, :, :]) / 2\n        m2c = (m2BC[:, :, :, :-1, :] + m2BC[:, :, :, 1:, :]) / 2\n        rhoc = (rhoBC[:, :, :, :, :-1] + rhoBC[:, :, :, :, 1:]) / 2\n\n        f = ((m1c ** 2 + m2c ** 2) / rhoc).mean()\n        return f\n```\n:::\n\n\nThis class models the dynamics of mass transport with momentum and density fields while ensuring\nmass conservation via the continuity equation. The learnable parameters defined here are $m_1$, $m_2$, and $\\rho$.\nThe specific class functions are OMTconstraintMF, OMTgradRho, and OMTobjfunMF. A common line you can notice is rhop = torch.exp(self.rho),\nwhich ensures that rhop is used instead of rho so as to let only positive values to the PDE system so as to conserve the convexity of optimization.\nOMTconstraintMF calculates the residual of mass conservation by first order finite difference, which is later\ndenoted as constraint(c). OMTgradRho calculates the gradients of the density, which is denoted by rhop.\nOMTobjfunMF calculates the objective functional (f) which is the kinetic energy as per Benamou-Brenier's formulation.\nIn this function, to control checkerboard oscillations, the variables are calculated at cell centres.\n\n## Function to Generate OMT Interpolation\n\n::: {#60081415 .cell execution_count=3}\n``` {.python .cell-code}\ndef LearnOMT(x0, x1, nt=20, device='cpu'):\n    t = torch.linspace(0, 1, nt)\n\n    eps = 1e-2\n\n    I1_min = x0.min()\n    I2_min = x1.min()\n    I1_mean = (x0 - I1_min + eps).mean()\n    I2_mean = (x1 - I2_min + eps).mean()\n\n    I1 = (x0 - I1_min + eps) / I1_mean\n    I2 = (x1 - I2_min + eps) / I2_mean\n\n    bs = I1.shape[0]\n    ch = I1.shape[1]\n    n1 = I1.shape[2]\n    n2 = I1.shape[3]\n    n3 = nt\n\n    omt = omtFD(n1, n2, n3, bs=bs * ch).to(device)\n\n    # input shape for omt: (bs, 1, nx, ny)\n    # final output shape for omt: (bs, 1, nx, ny, nt)\n\n    I1 = I1.reshape([bs * ch, 1, n1, n2])\n    I2 = I2.reshape([bs * ch, 1, n1, n2])\n\n    optimizer = optim.Adam(omt.parameters(), lr=1e-2)\n\n    num_epochs = 1000  # with incomplete optimization you do not see the transport, rather it appears as one dims and the other brightens up\n    opt_iter = 20\n    torch.autograd.set_detect_anomaly(True)\n\n    # Train the model\n    mu = 1e-2\n    delta = 1e-1\n\n    # initialize lagrange multiplier\n    p = torch.zeros(n1, n2, n3).to(device)\n    for epoch in range(num_epochs):\n        # Optimization iterations for fixed p\n        for jj in range(opt_iter):\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # function evaluation\n            f = omt.OMTobjfunMF(I1, I2)\n            # Constraint\n            c = omt.OMTconstraintMF(I1, I2)\n\n            const = -(p * c).mean() + 0.5 * mu * (c ** 2).mean()\n\n            loss = f + const\n\n            # Backward pass and optimize\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(omt.parameters(), clip_value=0.5)\n            optimizer.step()\n\n        #print('f:', f.item(), '\\tconst:', const.item())\n\n        # Update Lagrange multiplier\n        with torch.no_grad():\n            p = p - delta * mu * c\n\n    rho = omt.rho.detach()\n    rhop = torch.exp(rho)\n\n    # Add I1 and I2 to rho\n    rhop = torch.cat((I1.unsqueeze(-1), rhop, I2.unsqueeze(-1)), dim=-1)\n    rhop = rhop.reshape([bs, ch, n1, n2, n3 + 1])\n    rhop = rhop * I1_mean - eps + I1_min  # assuming mass of x0 = mass of x1\n    rhop = rhop.detach()   # shape of rhop: (bs, ch, nx, ny, nt+1) includes x0, \"nt-1\" timesteps, x1\n\n    return rhop\n```\n:::\n\n\nThis module starts with normalizations to match the masses of two images and add a background mass. The input tensors\nx0 and x1 (representing initial and final states) needs to have positive values and scale them to have a mean of 1.\nThis ensures numerical stability during the optimization. The Adam optimizer is set up to train the parameters of\nthe omtFD class.\n\n### Training Loop:\n\nThe training hyperparameters are defined: num_epochs (the number of optimization epochs), opt_iter (the number of inner iterations per epoch), and mu and delta (regularization parameters for the Lagrange multiplier).\nThe Lagrange multiplier p is initialized to enforce the mass conservation constraint. For each epoch, the following steps are performed for opt_iter iterations:\n\n* The OMT objective function (f) is computed using omt.OMTobjfunMF.\n* The mass conservation constraint (c) is evaluated using omt.OMTconstraintMF.\n* The total loss is computed as the sum of the objective (f) and the constraint penalty, weighted by p and mu.\n* Backpropagation is performed, gradients are clipped for stability, and the model parameters are updated.\n* The Lagrange multiplier p is updated gradually to enforce the constraint over epochs.\n\nOutput Density Interpolation:\nThe optimized density field rho is extracted from the model and exponentiated to compute the final density rhop. The initial and final states (I1 and I2) are concatenated with rhop to include all intermediate densities, forming the complete transport trajectory. The density trajectory rhop is rescaled back to the original input scale and format, ensuring mass conservation.\nThe interpolated density trajectory rhop is returned, with a shape of (batch_size, channels, n1, n2, nt+1). This includes the initial state (x0), the intermediate densities, and the final state (x1).\n\n## Running OMT\n\n::: {#1ba0f4c6 .cell execution_count=4}\n``` {.python .cell-code}\ndevice = 'cuda:0' # default: 'cpu'\n\nx0 = torch.load('x0.pt')\nx1 = torch.load('x1.pt')\n# shape of x0 and x1 = [:,:]\n\nsize_img = 64\nx0 = F.interpolate(x0.unsqueeze(0).unsqueeze(0), size = [size_img,size_img])\nx1 = F.interpolate(x1.unsqueeze(0).unsqueeze(0), size = [size_img,size_img])\n\nx0 = x0.to(device)\nx1 = x1.to(device)\n\nOMTInterpolation = LearnOMT(x0, x1, nt=10, device = device)\n# shape of Interpolation = [|B|,|C|,|X|,|Y|,|T|]\n```\n:::\n\n\n### Generate Linear Interpolation for Compare\n\n::: {#4155b13c .cell execution_count=5}\n``` {.python .cell-code}\nOMTInterpolation = OMTInterpolation[0,0].detach().cpu().numpy()\nx0 = x0[0,0].unsqueeze(-1)\nx1 = x1[0,0].unsqueeze(-1)\nFrac = torch.linspace(0,1,OMTInterpolation.shape[-1])\nFrac = Frac.unsqueeze(0).unsqueeze(0)\n\n# Linear Interpolation\nLinearInt = x1*Frac + (1-Frac)*x0\n\nLinearInt = LinearInt.detach().cpu().numpy()\n```\n:::\n\n\n## Visualization\n\n::: {#7d6effc4 .cell execution_count=6}\n``` {.python .cell-code}\n# shape of Interpolation = [|B|,|C|,|X|,|Y|,|T|]\nfig, ax = plt.subplots(nrows=2, ncols=OMTInterpolation.shape[-1], figsize=(50, 5))\n\nfor i in range(OMTInterpolation.shape[-1]):\n    ax[0, i].imshow(LinearInt[:, :, i])\n    ax[0, i].axis('off')\n    ax[1, i].imshow(OMTInterpolation[:, :, i])\n    ax[1, i].axis('off')\n\nax[0,0].set_title('Linear Interpolation', loc='left')\nax[1,0].set_title('OMT Interpolation', loc='left')\n\nplt.show()\n```\n:::\n\n\n# Conclusion\n\n![Comparison of linear interpolation with OMT interpolation](comparison.png){#fig-com}\n\nThe application of Benamou-Brenier's optimal mass transport (OMT) to model the shape progression demonstrates\nthe power of combining mathematical rigor with modern computational tools. By leveraging the\naugmented Lagrangian method for efficient PDE-constrained optimization and the GPU acceleration capabilities of\nPyTorch, we achieved a computationally viable approach to interpolating MRI images across time. This study\nhighlights the potential of OMT in predicting missing temporal states in medical imaging, such as generating\nplausible MRI scans at intermediate ages. Interpolation result can be observed in @fig-com.\nThe OMT interpolation looks very physical. Such advancements can contribute\nsignificantly to understanding shape morphing and interpolation. Future work could explore extending this approach\nto complex 2D shapes or 3D volumes like those in MRI scans, integrating other imaging modalities,\nand refining the method to address even larger datasets.\n\n",
    "supporting": [
      "OT4DiseaseProgression2_files"
    ],
    "filters": [],
    "includes": {}
  }
}