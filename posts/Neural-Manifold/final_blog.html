<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Deven Shidfar">
<meta name="dcterms.date" content="2024-09-18">

<title>Understanding Animal Navigation using Neural Manifolds With CEBRA – bioshape-analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c75bcb89907db7808f0349b764a1a524.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">bioshape-analysis</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Understanding Animal Navigation using Neural Manifolds With CEBRA</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">mathematics</div>
                <div class="quarto-category">biomedical engineering</div>
                <div class="quarto-category">neuroscience</div>
                <div class="quarto-category">biology</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliations</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Deven Shidfar <a href="mailto:devenshidfar@math.ubc.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              <a href="https://rtviii.xyz/">
              KDD Group
              </a>
            </p>
          <p class="affiliation">
              <a href="https://www.nc4.sbme.ubc.ca/">
              NC4 Lab
              </a>
            </p>
          <p class="affiliation">
              <a href="https://www.math.ubc.ca/">
              Department of Mathematics, UBC
              </a>
            </p>
          <p class="affiliation">
              <a href="https://bme.ubc.ca/">
              Department of Biomedical Engineering, UBC
              </a>
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 18, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#part-1-introduction" id="toc-part-1-introduction" class="nav-link active" data-scroll-target="#part-1-introduction">Part 1: Introduction</a>
  <ul class="collapse">
  <li><a href="#general-background" id="toc-general-background" class="nav-link" data-scroll-target="#general-background">General Background</a></li>
  <li><a href="#experimental-setup" id="toc-experimental-setup" class="nav-link" data-scroll-target="#experimental-setup">Experimental Setup</a>
  <ul class="collapse">
  <li><a href="#the-dome-apparatus" id="toc-the-dome-apparatus" class="nav-link" data-scroll-target="#the-dome-apparatus">The Dome Apparatus</a></li>
  <li><a href="#experiment-1-jayakumar-et-al.-2019" id="toc-experiment-1-jayakumar-et-al.-2019" class="nav-link" data-scroll-target="#experiment-1-jayakumar-et-al.-2019">Experiment 1: Jayakumar et al., 2019</a></li>
  <li><a href="#experiment-2-madhav-et-al.-2024" id="toc-experiment-2-madhav-et-al.-2024" class="nav-link" data-scroll-target="#experiment-2-madhav-et-al.-2024">Experiment 2: Madhav et al., 2024</a></li>
  </ul></li>
  <li><a href="#hippocampal-gain-definition" id="toc-hippocampal-gain-definition" class="nav-link" data-scroll-target="#hippocampal-gain-definition">Hippocampal Gain: Definition</a></li>
  <li><a href="#calculation-of-hippocampal-gain-and-current-limitations" id="toc-calculation-of-hippocampal-gain-and-current-limitations" class="nav-link" data-scroll-target="#calculation-of-hippocampal-gain-and-current-limitations">Calculation of Hippocampal Gain and Current Limitations</a>
  <ul class="collapse">
  <li><a href="#limitation-1" id="toc-limitation-1" class="nav-link" data-scroll-target="#limitation-1">Limitation 1</a></li>
  <li><a href="#limitation-2" id="toc-limitation-2" class="nav-link" data-scroll-target="#limitation-2">Limitation 2</a></li>
  </ul></li>
  <li><a href="#proposed-method" id="toc-proposed-method" class="nav-link" data-scroll-target="#proposed-method">Proposed Method</a></li>
  </ul></li>
  <li><a href="#part-2-methods" id="toc-part-2-methods" class="nav-link" data-scroll-target="#part-2-methods">Part 2: Methods</a>
  <ul class="collapse">
  <li><a href="#manifold-learning-technique--cebra" id="toc-manifold-learning-technique--cebra" class="nav-link" data-scroll-target="#manifold-learning-technique--cebra">Manifold Learning Technique -CEBRA</a></li>
  <li><a href="#persistent-homology" id="toc-persistent-homology" class="nav-link" data-scroll-target="#persistent-homology">Persistent Homology</a></li>
  <li><a href="#spline-parametrization-for-unsupervised-decoding-spud" id="toc-spline-parametrization-for-unsupervised-decoding-spud" class="nav-link" data-scroll-target="#spline-parametrization-for-unsupervised-decoding-spud">Spline parametrization for Unsupervised Decoding (SPUD)</a>
  <ul class="collapse">
  <li><a href="#parametrizing-the-latent-variable" id="toc-parametrizing-the-latent-variable" class="nav-link" data-scroll-target="#parametrizing-the-latent-variable">Parametrizing the Latent Variable</a></li>
  </ul></li>
  <li><a href="#decoding-hippocampal-gain-mathcalh" id="toc-decoding-hippocampal-gain-mathcalh" class="nav-link" data-scroll-target="#decoding-hippocampal-gain-mathcalh">Decoding Hippocampal Gain (<span class="math inline">\(\mathcal{H}\)</span>)</a></li>
  </ul></li>
  <li><a href="#part-3-results" id="toc-part-3-results" class="nav-link" data-scroll-target="#part-3-results">Part 3: Results</a>
  <ul class="collapse">
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a>
  <ul class="collapse">
  <li><a href="#h-values" id="toc-h-values" class="nav-link" data-scroll-target="#h-values">H values</a></li>
  </ul></li>
  <li><a href="#general-results" id="toc-general-results" class="nav-link" data-scroll-target="#general-results">General results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<style>
  .figure figcaption,
  .cell .figure .caption {
    color: #777;
    font-size: 0.9em;
    text-align: center;
    margin-top: 0.5em;
  }
  
  /* For grid containers */
  .grid .figure figcaption,
  .grid .figure .caption {
    color: #777;
    font-size: 0.9em;
    text-align: center;
    margin-top: 0.5em;
  }
</style>
<section id="part-1-introduction" class="level1">
<h1>Part 1: Introduction</h1>
<section id="general-background" class="level2">
<h2 class="anchored" data-anchor-id="general-background">General Background</h2>
<p>Seeing, hearing, touching – every moment, our brain receives numerous sensory inputs. How does it organize this wealth of data and extract relevant information? We know that the brain forms a coherent neural representation of the external world called the cognitive map (<span class="citation" data-cites="tolman1948">Tolman (<a href="#ref-tolman1948" role="doc-biblioref">1948</a>)</span>), formed by the combined firing activity of neurons in the hippocampal formation. For example, place cells are neurons that fire when a rat is at a particular location (<span class="citation" data-cites="moser2008">Moser, Kropff, and Moser (<a href="#ref-moser2008" role="doc-biblioref">2008</a>)</span>). Together, the activity of hundreds of these place cells can be modeled as a manifold, mapping the rat’s location in physical space. More specifically, the hippocampus creates such a cognitive map through “path integration” of various cues, such as optic flow, vestibular inputs, and proprioception to keep track of an animal’s position.</p>
<p>To provide insights on how the brain generates such cognitive maps, manifold learning has been used to extract lower-dimensional geometric representations from high- dimensional neural data (<span class="citation" data-cites="mitchell-heggs2023">Mitchell-Heggs et al. (<a href="#ref-mitchell-heggs2023" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="schneider2023">Schneider, Lee, and Mathis (<a href="#ref-schneider2023" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="chaudhuri2019">Chaudhuri et al. (<a href="#ref-chaudhuri2019" role="doc-biblioref">2019</a>)</span>).</p>
<p><strong>The question then arises:</strong> Can we decode important navigational behavioural variables during an experiment through manifold learning? In this blog post, we will focus on developing a new procedure for learning a neural manifold from specific navigation experiments (<span class="citation" data-cites="madhav2024">Madhav et al. (<a href="#ref-madhav2024" role="doc-biblioref">2024</a>)</span>, <span class="citation" data-cites="jayakumar2019">Jayakumar et al. (<a href="#ref-jayakumar2019" role="doc-biblioref">2019</a>)</span>) that will allow us to improve the decoding of an important navigational behvaioural variable. We first need to introduce the experimental setup.</p>
</section>
<section id="experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup">Experimental Setup</h2>
<div id="vr_dome" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/dome_apparatus.jpeg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Fig. 1 - Virtual reality Dome apparatus. Rats ran on a circular table surrounded by a hemispherical shell. A projector image reflects off a hemispherical mirror onto the inner surface of the shell. Image taken from <span class="citation" data-cites="madhav2024">Madhav et al. (<a href="#ref-madhav2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</div>
<section id="the-dome-apparatus" class="level3">
<h3 class="anchored" data-anchor-id="the-dome-apparatus">The Dome Apparatus</h3>
<p>Figure 1 illustrates the experimental apparatus, known as the Virtual Reality Dome. In the experiment, rats ran on a circular platform surrounded by a hemispherical projection surface called the Dome. Electrodes were inserted into the <strong>CA1</strong> of the hippocampus of <strong>male evan’s rats</strong> and spike rate neural activity of hippocampal place cells was recorded during the experiment. Hippocampal place cells (referred to as simply “place cells” from here on) are neurons in the hippocampus that fire when an animal is in a specific location.</p>
<p>To better understand place cells, let’s consider a simple example. Imagine a rat moving along a horizontal linear track. Suppose the rat has only <strong>three</strong> place cells. In this scenario:</p>
<ul>
<li>Neuron 1 fires when the rat is at the very left of the track.</li>
<li>Neuron 2 fires when the rat is in the middle of the track.</li>
<li>Neuron 3 fires when the rat is at the very right of the track.</li>
</ul>
<p>As the rat moves along the track, specific place cells activate depending on the rat’s location. This pattern of firing enables the rat to construct an internal cognitive map of its environment. Such spatial encoding is important for navigation and memory formation.</p>
<p>In this blog, we will analyze data from two experiments that examine place cell activity under controlled conditions. While both experiments share similarities in how brain activity was recorded and analyzed, they differ in their use of external sensory cues.</p>
</section>
<section id="experiment-1-jayakumar-et-al.-2019" class="level3">
<h3 class="anchored" data-anchor-id="experiment-1-jayakumar-et-al.-2019">Experiment 1: Jayakumar et al., 2019</h3>
<p>This experiment used a virtual reality dome (<span class="citation" data-cites="madhav2022">Madhav et al. (<a href="#ref-madhav2022" role="doc-biblioref">2022</a>)</span>) which projects visual landmarks onto the interior of the dome. The experimenters have the ability to displace these landmarks systematically. The experimental gain, denoted as <span class="math inline">\(\mathcal{G}\)</span> quantifies the relationship between the rat’s movement and the displacement of these landmarks, creating controlled mismatches between self-motion cues and feedback from external landmarks.</p>
<p>Key Landmark Gain Conditions:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{G}\)</span> = 1: Landmarks remain stationary in the lab frame, with the rat traveling the same distance in both the landmark and lab frames.</p></li>
<li><p><span class="math inline">\(\mathcal{G}\)</span> &gt; 1: Landmarks move opposite to the rat’s direction, the rat runs a greater distance in the landmark frame than in the lab frame. This causes the rat to perceive itself as having travelled a greater distance than it actually has.</p></li>
<li><p><span class="math inline">\(\mathcal{G}\)</span> &lt; 1: Landmarks move in the same direction as the rat but more slowly, hence the rat runs a shorter distance in the landmark frame than in the lab frame. This causes the rat to perceive itself as having travelled a shorter distance than it actually has.</p></li>
</ul>
</section>
<section id="experiment-2-madhav-et-al.-2024" class="level3">
<h3 class="anchored" data-anchor-id="experiment-2-madhav-et-al.-2024">Experiment 2: Madhav et al., 2024</h3>
<p>In this follow-up study, the virtual reality dome projected moving stripes on the interior to provide controlled optic flow cues. The movement of these stripes was coupled to the rat’s movement, with the stripe gain (<span class="math inline">\(\mathcal{S}\)</span>) determining the relationship between the rat’s speed and the stripes’ speed. This setup allowed for controlled distortions of the rat’s perception of self-motion.</p>
<p>Key Stripe Gain Conditions:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{S}\)</span> = 1: Stripes are stationary relative to the lab frame, meaning the rat is not recieving conflicting cues.</p></li>
<li><p><span class="math inline">\(\mathcal{S}\)</span> &gt; 1: Stripes move opposite to the rat’s direction, causing the rat to percieve itself as moving faster than it is.</p></li>
<li><p><span class="math inline">\(\mathcal{S}\)</span> &lt; 1: Stripes move in the same direction but slower than the rat, causing the rat to percieve itself as moving slower than it is.</p></li>
</ul>
<p>For example, if a rat ran counterclockwise (CCW), then with S = 2, the stripes moved clockwise (CW) at the same speed as the rat. Likewise, with S = 0.5, the stripes moved CCW at half the speed of the rat.</p>
</section>
</section>
<section id="hippocampal-gain-definition" class="level2">
<h2 class="anchored" data-anchor-id="hippocampal-gain-definition">Hippocampal Gain: Definition</h2>
<p>For both experiments, Dr.&nbsp;Madhav and colleagues introduce a value called the hippocampal gain, <span class="math inline">\(\mathcal{H}\)</span> defined as:</p>
<p><span class="math display">\[
  \mathcal{H} = \frac{\text{distance travelled in hippocampal reference frame}}{\text{distance travelled in lab reference frame}}.
\]</span></p>
<p>It quantifies the relationship between the rat’s distance travelled in the internal hippocampal frame versus the lab frame. The distance travelled in the lab reference frame is simply the true distance travelled in physical space. The distance travelled in the hippocampal reference frame can be thought of as the distance the rat “perceives” itself to be moving from its place cell activity - essentially the “distance” represented in the neural activity.</p>
<ul>
<li><p><span class="math inline">\(\mathcal{H} = 1\)</span>: The rat perceives itself as moving the “correct” speed.</p></li>
<li><p><span class="math inline">\(\mathcal{H} &gt; 1\)</span>: The rat perceives itself as moving faster than it actually is with respect to the lab frame.</p></li>
<li><p><span class="math inline">\(\mathcal{H} &lt; 1\)</span>: The rat perceives itself as moving slower than it actually is with respect to the lab frame.</p></li>
</ul>
<p><span class="math inline">\(\mathcal{H}\)</span> gives valuable insights into how these visual cues (optic flow cues or landmark cues) affect the rats’ internal representation during the task. It gives an understanding of how the rats update their perceived position in the environment.</p>
<p>For example, an <span class="math inline">\(\mathcal{H}\)</span> value of 2 would mean that the rat perceives itself as moving twice as fast as it actually is. Consequently each place cell corresponding to a specific location in the maze will fire twice per lap rather than once.</p>
<p>For the remainder of this blog, we will refer to both lab frame and hippocampal position as lab frame angle, and hippocampal angle, respectively. This is because the rat is moving on the perimeter of a circular dome, making angle and position effectively interchangeable. As we will see, referring to it as an angle simplifies the discussion and makes more sense given the geometry of the setup.</p>
</section>
<section id="calculation-of-hippocampal-gain-and-current-limitations" class="level2">
<h2 class="anchored" data-anchor-id="calculation-of-hippocampal-gain-and-current-limitations">Calculation of Hippocampal Gain and Current Limitations</h2>
<p><strong>Method of Determining <span class="math inline">\(\mathcal{H}\)</span></strong>: In the two papers discussed earlier (<span class="citation" data-cites="jayakumar2019">Jayakumar et al. (<a href="#ref-jayakumar2019" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="madhav2024">Madhav et al. (<a href="#ref-madhav2024" role="doc-biblioref">2024</a>)</span>), <span class="math inline">\(\mathcal{H}\)</span> was determined by analyzing the spatial periodicity of place cell firing over multiple laps using Fourier transforms.</p>
<p><br></p>
<div id="spectral_decoding" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/spectral_decode.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Figure 2 - (a) Spectral decoding algorithm. In the dome, as visual landmarks are presented and moved at an experimental gain G, the rat encounters a particular landmark every 1/G laps (the spatial period). If the place fields fire at the same location in the landmark reference frame, the firing rate of the cell exhibits a spatial frequency of G fields per lap. a, Illustration of place-field firing for three values of hippocampal gain, H. Taken from <span class="citation" data-cites="madhav2024">Madhav et al. (<a href="#ref-madhav2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</div>
<p><br></p>
<p>In Figure 2 we see an illustration of how the Fourier transform method of decoding <span class="math inline">\(\mathcal{H}\)</span> is performed. The spatial frequency of firing for each place cell effectively decodes the <span class="math inline">\(\mathcal{H}\)</span> value for that specific neuron, and the mean <span class="math inline">\(\mathcal{H}\)</span> value over all neurons gives the estimated <span class="math inline">\(\mathcal{H}\)</span> value over the neuronal population.</p>
<section id="limitation-1" class="level3">
<h3 class="anchored" data-anchor-id="limitation-1">Limitation 1</h3>
<p>This method lacks temporal precision within individual laps since it uses a Fourier Transform over 6 laps.</p>
<p>A more precise, within-lap decoding of <span class="math inline">\(\mathcal{H}\)</span> could provide a deeper understanding of how path integration occurs with finer temporal resolution. This could lead to new insights into how the brain updates its neural representation when receiving conflicting visual cues.</p>
</section>
<section id="limitation-2" class="level3">
<h3 class="anchored" data-anchor-id="limitation-2">Limitation 2</h3>
<p>Note how the decoding of <span class="math inline">\(\mathcal{H}\)</span> is directly tied to the neural data, i.e.&nbsp;the periodicity of neuronal spikes. Now, imagine a scenario where, instead of a single varying neural representation <span class="math inline">\(\mathcal{H}\)</span>, we have two - <span class="math inline">\(\mathcal{H}\)</span> and another representation, <span class="math inline">\(\mathcal{A}\)</span>. In such cases, these two representations would be coupled in the neural data, making it impossible to disentangle them using the traditional approach.</p>
<p>However, neural manifold learning offers a promising approach to decouple these representations. For instance, consider the hypothetical scenario below, where the data forms a torus.</p>
<div class="grid figure">
<div class="g-col-6">
<p><img src="images/varying_spatial.jpg" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><img src="images/varying_audio.jpg" class="img-fluid" style="width:100.0%"></p>
</div>
</div>
<div class="cell figure">
<figcaption class="caption">
Figure 3 - Left: varying spatial representation (<span class="math inline">\(\mathcal{H}\)</span>), Right: varying audio representation (<span class="math inline">\(\mathcal{A}\)</span>).
</figcaption>
</div>
<p>In our current dataset, we have a single varying neural representation and therefore expect a simple 1D ring topology. However, as seen in Figure 3, the data might lie on a torus. On this structure, the spatial representation (<span class="math inline">\(\mathcal{H}\)</span>) could vary along the major circle of the torus, while the auditory representation (<span class="math inline">\(\mathcal{A}\)</span>) could vary along the minor circle. The proposed method would enable us to disentangle and decode the two neural representations independently. We wish to validate the new method for single varying representations, and then experiment with two varying neural representations.</p>
</section>
</section>
<section id="proposed-method" class="level2">
<h2 class="anchored" data-anchor-id="proposed-method">Proposed Method</h2>
<p>Our main goal is therefore to determine this <span class="math inline">\(\mathcal{H}\)</span> value without using a Fourier Transform and instead somehow find a method of determining <span class="math inline">\(\mathcal{H}\)</span> which is not tied directly to the neural data. We will achieve this by applying a specific manifold learning tehnique.</p>
<p>The basic idea is as follows: 1. First, we aim to project the neural data into some latent space. In this space, we want the points to map out the topology of the task - specifically, to encode hippocampal angle. 3. We want to validate that this task forms a 1D ring topology 4. We want to validate and construct a latent parametrization of this manifold, specifically designed to directly reflect the hippocampal angle. 5. With an accurate hippocampal angle parametrization, we then decode <span class="math inline">\(\mathcal{H}\)</span> using the original equation, giving us a more temporally fine estimation of <span class="math inline">\(\mathcal{H}\)</span>.</p>
</section>
</section>
<section id="part-2-methods" class="level1">
<h1>Part 2: Methods</h1>
<section id="manifold-learning-technique--cebra" class="level2">
<h2 class="anchored" data-anchor-id="manifold-learning-technique--cebra">Manifold Learning Technique -CEBRA</h2>
<p>CEBRA, introduced in <span class="citation" data-cites="schneider2023">Schneider, Lee, and Mathis (<a href="#ref-schneider2023" role="doc-biblioref">2023</a>)</span>, is a powerful self-supervised learning algorithm designed to create consistent, interpretable embeddings of high-dimensional neural recordings using auxiliary variables such as behavior or time. CEBRA generates consistent embeddings across trials, animals, and even different recording modalities​. In our analysis, we will use the discovery mode of CEBRA, with only time as our auxiliary variable. CEBRA is implemented in python.</p>
<p>In neuroscience, understanding how neural populations encode behavior is a large challenge. Traditional linear methods like PCA, or even non-linear approaches like UMAP and t-SNE, fail in this context because they fail to capture temporal dynamics and lack consistency across different sessions or animals. CEBRA gets past these limitations by both considering temporal dynamics and providing consistency across different sessions or animals.</p>
<p><br></p>
<div id="cebra_pipeline" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/CEBRA_pipeline.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Figure 4 - CEBRA Architecture. Input: Neural spike data in the shape (time points, neuron #). Output: Low dimensional embedding. Taken from <span class="citation" data-cites="schneider2023">Schneider, Lee, and Mathis (<a href="#ref-schneider2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</div>
<p><br></p>
<p>Figure 4 is a schematic showing the CEBRA architecture. CEBRA uses a convolutional neural network (CNN) encoder trained with contrastive learning to produce a latent embedding of the neural data.</p>
<p>The CEBRA model is trained using a contrastive learning loss function. In CEBRA, this is achieved through InfoNCE (Noise Contrastive Estimation), which encourages the model to distinguish between similar (positive) and dissimilar (negative) samples, given some similariy measure.</p>
<p>The loss function is defined as: <span class="math display">\[
\mathcal{L} = - \log \frac{e^{\text{sim}(f(x), f(y^+)) / \tau}}{e^{\text{sim}(f(x), f(y^+)) / \tau} + \sum_{i=1}^{K} e^{\text{sim}(f(x), f(y_i^-)) / \tau}}
\]</span></p>
<p>Where <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(y)\)</span> are the encoded representations of the neural data after passing through the CNN, <span class="math inline">\(\text{sim}(f(x), f(y))\)</span> represents a similarity measure between the two embeddings, implemented as cosine similarity. Here, <span class="math inline">\(y^{+}\)</span> denotes the positive pair (similar to <span class="math inline">\(x\)</span> in time), <span class="math inline">\(y_{i}^{-}\)</span> denotes the negative pairs (dissimilar to <span class="math inline">\(x\)</span> in time), and <span class="math inline">\(\tau\)</span> is a temperature parameter that controls the sharpness of the distribution.</p>
<p>Note that the similarity measure depends on the CEBRA mode used, and we have used time as our similarity measure. This way, the embeddings reflect the temporal structure of the data. The final output is then the embedding value in the latent space.</p>
<p>Once we obtain the neural embeddings from CEBRA, the next step is to determine the underlying manifold that describes the structure of the resulting point cloud. For example, let’s consider the output of a CEBRA embedding in 3D space from one experimental session.</p>
<p><br></p>
<div id="hipp_angle_no_curve" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hipp_session_50.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Figure 5 - CEBRA Embedding for an experiment with hippocampal angle annotated as a color map</figcaption>
</figure>
</div>
</div>
<p><br></p>
<p>The embedding appears to form a 1D circle in 3D space, which is consistent with our prediction that the neural activity encodes the hippocampal reference frame angle rather than the lab frame angle. We now want to validate quanititatively that our task forms a 1D ring topology. To do this, we apply a technique known as Persistent Homology.</p>
</section>
<section id="persistent-homology" class="level2">
<h2 class="anchored" data-anchor-id="persistent-homology">Persistent Homology</h2>
<p>In order to understand Persistent homology, we first need to understand how the topology of a manifold is described. Betti numbers describe the topological features of a space, with the <span class="math inline">\(k\)</span>-th Betti number counting the number of <span class="math inline">\(k\)</span>-dimensional “holes” in the manifold.</p>
<p><br></p>
<div id="betti_numbers" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/betti_numbers_illustrate.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Figure 6 - Some simple topological spaces and their Betti numbers. Taken from <span class="citation" data-cites="walker2008">Walker (<a href="#ref-walker2008" role="doc-biblioref">2008</a>)</span></figcaption>
</figure>
</div>
</div>
<p><br></p>
<p>Figure 6 shows a few basic topological spaces and their corresponding Betti numbers. Thus, for a 1D ring, the expected Betti numbers are:</p>
<p><span class="math display">\[
\beta_0 = 1 : \text{One connected component.}
\]</span> <span class="math display">\[
\beta_1 = 1 : \text{One 1D hole (i.e., the circular loop).}
\]</span> <span class="math display">\[
\beta_2 = 0 : \text{No 2D voids.}
\]</span></p>
<p>Thus, the expected Betti numbers for our manifold are (1, 1, 0). If the Betti numbers extracted from the persistent homology analysis align with these values, it confirms that the neural dynamics trace a 1D ring.</p>
<p>We now turn to persistent homology and how it helps us determine the Betti numbers of our manifold.</p>
<div id="betti_numbers" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/persistent_cohomology.jpeg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Figure 7 - Schematic showing how persistent cohomology is computed. Each data point is thickened to a ball of gradually expanding radius (r) while tracking the birth and death of ‘cycles’ in each dimension. Prominent lifespans, indicated by pink and purple arrows, are considered to determine Betti numbers. Taken from <span class="citation" data-cites="schneider2023">Schneider, Lee, and Mathis (<a href="#ref-schneider2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</div>
<p>The idea of persistent homology is to create spheres of varying radii around each point in the point cloud, and from those spheres, track how the topological features of the shape change as the radius grows. By systematically increasing the radii, we can observe when distinct points merge, when loops (1D holes) appear, and when higher-dimensional voids form. These features persist across different radius sizes, and their persistence provides a measure of their significance. In the context of neural data, this allows us to detect the underlying topological structure of the manifold.</p>
</section>
<section id="spline-parametrization-for-unsupervised-decoding-spud" class="level2">
<h2 class="anchored" data-anchor-id="spline-parametrization-for-unsupervised-decoding-spud">Spline parametrization for Unsupervised Decoding (SPUD)</h2>
<p>Once we’ve validated the assumption that our data forms a 1D ring manifold, we can proceed to fitting a spline to the data. We do this so that we can parametrize our behavioural variable, hippocampal angle (<span class="math inline">\(\theta_{H}\)</span>) along the point cloud. There are many different methods, but the one chosen for this purpose was taken from <span class="citation" data-cites="chaudhuri2019">Chaudhuri et al. (<a href="#ref-chaudhuri2019" role="doc-biblioref">2019</a>)</span>. The spline is defined by a set of points, or knots, which we decide to initialize using kmedoids clustering to the point cloud(<span class="citation" data-cites="jin2011">Jin and Han (<a href="#ref-jin2011" role="doc-biblioref">2011</a>)</span>). The knots are then fit to the data over <span class="math inline">\(\mathcal{k}\)</span> steps by optimizing an objective function defined as follows:</p>
<p><span class="math display">\[
\text{cost} = \text{dist} + \text{curvature} + \text{length} - \text{log(density)},
\]</span></p>
<p>where <em>dist</em> is the distance of each point to the spline, <em>curvature</em> is the total curvature of the spline, <em>length</em> is the total length of the spline, and <em>density</em> is the point cloud density of each knot.</p>
<section id="parametrizing-the-latent-variable" class="level3">
<h3 class="anchored" data-anchor-id="parametrizing-the-latent-variable">Parametrizing the Latent Variable</h3>
<p><span class="citation" data-cites="chaudhuri2019">Chaudhuri et al. (<a href="#ref-chaudhuri2019" role="doc-biblioref">2019</a>)</span> demonstrated that this method works for head direction cells in mice to accurately parametrize, i.e.&nbsp;decode the head direction. Our goal is to have the parametrization accurately decode our latent variable of interest, <span class="math inline">\(\theta_{H}\)</span>.</p>
<p>The method of parametrization we used is known as a natural parametrization. A natural parametrization ensures that equal distances in the embedding space correspond to equal changes in the latent variable. This comes from the idea that neural systems allocate resources proportional to the significance or frequency of stimuli. For example, in the visual cortex, frequently occurring stimuli, such as vertical or horizontal orientations, may be encoded with greater resolution. Conversely, in systems like place cell firing, where all angles or angles in a dome are equally probable, the natural parametrization reflects a uniform encoding strategy, avoiding overrepresentation of specific locations (<span class="citation" data-cites="chaudhuri2019">Chaudhuri et al. (<a href="#ref-chaudhuri2019" role="doc-biblioref">2019</a>)</span>).</p>
<div id="hipp_angle_with_curve" class="cell figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hipp_angle.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Figure 8 - Spline fit on CEBRA embedding. Note the alignment of the colors on the spline and the point clouds. This alignment means we are accurately parametrizing our latent variable of interest.</figcaption>
</figure>
</div>
</div>
<p>In Figure 8, we demonstrate this by fitting a spline to the CEBRA embedding, with data points colored according to their corresponding hippocampal angles. The results show a near-perfect alignment between the spline and the hippocampal angles, precisely the outcome we aimed for.</p>
<p>Next, we demonstrate how the parametrization allows us to arrive at our primary objective: decoding <span class="math inline">\(\mathcal{H}\)</span>.</p>
</section>
</section>
<section id="decoding-hippocampal-gain-mathcalh" class="level2">
<h2 class="anchored" data-anchor-id="decoding-hippocampal-gain-mathcalh">Decoding Hippocampal Gain (<span class="math inline">\(\mathcal{H}\)</span>)</h2>
<p>The final step is to decode <span class="math inline">\(\mathcal{H}\)</span> from the parametrization. The method to do this is straightforward. Once we have parametrized the spline accurately to the neural data, we calculate the hippocampal gain by comparing the distance travelled in the neural manifold (derived from our spline, which gives us hippocampal angle) to the distance travelled in the lab frame (actual movement of the rat).</p>
<p>The idea is that:</p>
<p><span class="math display">\[
\mathcal{H} = \frac{d\theta_\mathcal{H}}{d\theta_\mathcal{L}}
\]</span></p>
<p>where <span class="math inline">\(\theta_H\)</span> is the angle in the hippocampal reference frame, decoded from our spline parametrization of the neural manifold, and <span class="math inline">\(\theta_L\)</span> is the angle of the rat in the lab frame.</p>
<p>Note that this is actually just the original definition of <span class="math inline">\(\mathcal{H}\)</span>, but now <span class="math inline">\(\theta_H\)</span> is determined by our spline parameter, not the Fourier transform method.</p>
<p>Let’s consider a specific time interval, such as 1–2 seconds, to demonstrate the application of this equation. To calculate the hippocampal gain within this interval, we start by identifying where the neural activity at times 1 and 2 maps onto the spline parameterization of our manifold, denoting these angles as <span class="math inline">\(\theta_{H1}\)</span> and <span class="math inline">\(\theta_{H2}\)</span>, respectively. Simultaneously, we observe the lab frame angles at the same times, referred to as <span class="math inline">\(\theta_{L1}\)</span> and <span class="math inline">\(\theta_{L2}\)</span>. With these values, the hippocampal gain between t=1 and t=2 is determined as:</p>
<p><span class="math display">\[
  \mathcal{H}(\text{between } t=1 \text{ and } t=2) = \frac{\theta_{\mathcal{H2}} - \theta_{\mathcal{H1}}}{\theta_{\mathcal{L2}} - \theta_{\mathcal{L1}}}
\]</span></p>
<p>We extend the above example to all consecutive time points in the experiment to compute hippocampal gain (<span class="math inline">\(\mathcal{H}\)</span>) over the course of the experiment. The following Python code demonstrates how this is implemented:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> differentiate_and_smooth(data<span class="op">=</span><span class="va">None</span>, window_size<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute finite differences.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    diffs <span class="op">=</span> np.diff(data)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the moving average of differences.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    kernel <span class="op">=</span> np.ones(window_size) <span class="op">/</span> window_size</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    avg_diffs <span class="op">=</span> np.convolve(diffs, kernel, mode<span class="op">=</span><span class="st">'valid'</span>) </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_diffs</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>derivative_decoded_angle <span class="op">=</span> differentiate_and_smooth(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>filtered_decoded_angles_unwrap, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span><span class="dv">60</span> <span class="co"># Hippocampal angle from manifold parametrization.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>derivative_true_angle <span class="op">=</span> differentiate_and_smooth(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>binned_true_angle_rad_unwrap, </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span><span class="dv">60</span> <span class="co"># True angle from session recordings.</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>decode_H <span class="op">=</span> derivative_decoded_angle <span class="op">/</span> derivative_true_angle</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code calculates the hippocampal gain, <span class="math inline">\(\mathcal{H}\)</span>, by dividing the finite differences of the hippocampal angle (obtained from the manifold parametrization) by the derivative of the true angle (obtained from session recordings). The result can be compared to <span class="math inline">\(\mathcal{H}\)</span> determined from the traditional Fourier-based method, shown in the results section.</p>
</section>
</section>
<section id="part-3-results" class="level1">
<h1>Part 3: Results</h1>
<section id="datasets" class="level2">
<h2 class="anchored" data-anchor-id="datasets">Datasets</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Data Qualities</th>
<th>Landmark Experiment Dataset (<span class="citation" data-cites="jayakumar2019">Jayakumar et al. (<a href="#ref-jayakumar2019" role="doc-biblioref">2019</a>)</span>)</th>
<th>Optic Flow Experiment Dataset (<span class="citation" data-cites="madhav2024">Madhav et al. (<a href="#ref-madhav2024" role="doc-biblioref">2024</a>)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Number of Trials</td>
<td>72</td>
<td>65</td>
</tr>
<tr class="even">
<td>Data Access</td>
<td>Available upon request</td>
<td>Available at: https://doi.org/10.7281/T1/THLC8N</td>
</tr>
<tr class="odd">
<td>Average Number of Recorded Neurons</td>
<td>10</td>
<td>33</td>
</tr>
</tbody>
</table>
<p>As discussed in <a href="##experimental-setup">Experimental Setup</a>, there are two types of experiments. Their datasets are summarized above.</p>
</section>
<section id="examples" class="level2">
<h2 class="anchored" data-anchor-id="examples">Examples</h2>
<p>In this section, we present and analyze the results. In some trials, the data failed to form a clear 1D ring topology, as evident from the spline parametrization. This can be quantitatively evaluated using persistent homology. In the next section we will provide a more detailed explanation of what constitutes “good” versus “bad” trials.</p>
<div class="grid">
<div class="g-col-6">
<p><strong>a</strong> <img src="images/hipp_session_50_princ.png" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><strong>c</strong> <img src="images/hipp_session_26_princ.png" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><strong>b</strong> <img src="images/hipp_session_36_princ.png" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><strong>d</strong> <img src="images/hipp_session_29_princ.png" class="img-fluid" style="width:100.0%"></p>
</div>
</div>
<div class="cell figure">
<figcaption class="caption">
Figure 9 - Embeddings for both successful and unsuccessful trials: (a) Session 50, (b) Session 36, (c) Session 26, and (d) Session 29 show embeddings with the spline fit (in red). (a) and (b) represent successful trials, whereas (c) and (d) represent unsuccessful trials where the manifold does not form a clear 1D ring topology.
</figcaption>
</div>
<p>The examples in Figure 9 showcase the application of our method to experimental data from “Control and recalibration of path integration in place cells” (<span class="citation" data-cites="madhav2024">Madhav et al. (<a href="#ref-madhav2024" role="doc-biblioref">2024</a>)</span>). Specifically, we show two “good” trials (session 50 and 36) and two “bad” trials (session 26 and 29).</p>
<section id="h-values" class="level3">
<h3 class="anchored" data-anchor-id="h-values">H values</h3>
<div class="grid">
<div class="g-col-6">
<p><strong>a</strong> <img src="images/H_session_50.png" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><strong>c</strong> <img src="images/H_session_26.png" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><strong>b</strong> <img src="images/H_session_36.png" class="img-fluid" style="width:100.0%"></p>
</div>
<div class="g-col-6">
<p><strong>d</strong> <img src="images/H_session_29.png" class="img-fluid" style="width:100.0%"></p>
</div>
</div>
<div class="cell figure">
<figcaption class="caption">
Figure 10 - Plot of manifold-decoded gain (red line) vs.&nbsp;gain from the traditional method (blue line) for different sessions: (a) Session 50, (b) Session 36, (c) Session 26, and (d) Session 29. We want the red line to track the blue line, as this tells us that we have accurately decoded <span class="math inline">\(\mathcal{H}\)</span> using our new method.
</figcaption>
</div>
<p>After observing both successful and unsuccessful trials, we sought to indentify the factors that differentiate “good” results from “bad” ones.</p>
<p>To quantify the quality of an embedding, we used the <strong>Structure Index (SI) score</strong> (<span class="citation" data-cites="sebastian2022">Sebastian, Esparza, and Prida (<a href="#ref-sebastian2022" role="doc-biblioref">2022</a>)</span>). The SI score measures how well the hippocampal angle is distributed across the point cloud. Using the SI score, It became evident that the quality of results was strongly influenced by the <strong>number of neurons</strong> in the experimental recording.</p>
<ul>
<li><strong>SI ranges from 0 to 1:</strong>
<ul>
<li><strong>0:</strong> The hippocampal angle is randomly distributed among the point cloud.</li>
<li><strong>1:</strong> The hippocampal angle is perfectly distributed among the point cloud, indicating a clear and accurate representation.</li>
</ul></li>
</ul>
<p>Thus, a higher SI score corresponds to a better alignment between the hippocampal angle and the manifold parametrization.</p>
<p>Consider the trials discussed earlier:</p>
<ul>
<li><strong>Successful trials (Sessions 50 and 36):</strong> SI scores were <strong>0.89</strong> and <strong>0.90</strong>, respectively.</li>
<li><strong>Unsuccessful trials (Sessions 26 and 29):</strong> SI scores were <strong>0.34</strong> and <strong>0.67</strong>, respectively.</li>
</ul>
<div id="clust_SI" class="cell figure" data-fig-align="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/num_clust_SI_linear.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Figure 11 - Relationship between number of neurons and SI score.</figcaption>
</figure>
</div>
</div>
<p>Figure 11 illustrates the relationship between the number of neurons and the SI score. This highlights what we refer to as the “<strong>curse of neurons</strong>”: A minimum number of neurons is required to achieve a successful result.</p>
</section>
</section>
<section id="general-results" class="level2">
<h2 class="anchored" data-anchor-id="general-results">General results</h2>
<p>In Figure 11, we see that trials with fewer neurons (&lt;35 neurons) are more likely to fail, while those with more neurons (&gt;35 neurons) generally produce high-quality embeddings with accurate parametrization.</p>
<p>Look at the plot below, where we look at the relationship between number of neurons and mean <span class="math inline">\(\mathcal{H}\)</span> decode error, which is calculated as,</p>
<p><span class="math display">\[
\text{mean} \, \mathcal{H} \, \text{decode error} = \frac{1}{n} \sum_{i=1}^{n} \left( H_{\text{decode}}[i] - H_{\text{fourier}}[i] \right),
\]</span></p>
<p>where the sum is taken over all time indices <span class="math inline">\(\mathcal{i}\)</span> in each array, and n is the number of time points.</p>
<div id="hipp_angle_with_curve" class="cell figure" data-fig-align="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/num_neurons_vs_mean_H_box.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Figure 12 - Plot of number of neurons vs mean <span class="math inline">\(\mathcal{H}\)</span> error, grouped by trials with either less or more than 35 neurons.</figcaption>
</figure>
</div>
</div>
<p>We can observe from Figure 12 that the error for experiments with more than 35 neurons is generally lower than the error for experiments with fewer than 35 neurons. However, it is less evident from the figure that the majority of trials in the &gt;35 neuron group have an error value of less than 0.01. This leads us to the next figure.</p>
<div id="hipp_angle_with_curve" class="cell figure" data-fig-align="center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/num_clusters_vs_mean_H_error.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Figure 13 - Plot of number of neurons vs mean <span class="math inline">\(\mathcal{H}\)</span> error, grouped by trials with either less or more than 35 neurons.</figcaption>
</figure>
</div>
</div>
<p>From figure 13 we can see that the majority of trials with more than 35 neurons have a mean <span class="math inline">\(\mathcal{H}\)</span> decode error of less than 0.01. However, certain trials with more than 35 neurons show higher decoding errors. This is because CEBRA, our method of manifold learning, does not explicitly force a 1D ring topology. The method often produces a 1D ring due to the task and its objective. However, in certain trials, it may converge to a different configuration, such as distributing the point cloud across a sphere, even when we have &gt;35 neurons and a high SI score. Our method is not equipped to handle data that deviates from a 1D ring task topology, which is reflected in those trials where the <span class="math inline">\(\mathcal{H}\)</span> error exceeds 0.01.</p>
<p>In other words, what we ideally want from CEBRA is the following:</p>
<p><span class="math display">\[
\mathcal{H} \text{ error } &lt; \epsilon \iff \text{&gt;35 neurons in our experiment},
\]</span></p>
<p>i.e, we want a guarantee that if we have a certain amount of neurons recorded in our experiment, then we will get reasonable results. Whereas what we currently observe is,</p>
<p><span class="math display">\[
\mathcal{H} \text{ error } &lt; \epsilon \implies \text{&gt;35 neurons in our experiment}, \text{ but, }  
\text{&gt;35 neurons in our experiment} \not\implies \mathcal{H} \text{ error } &lt; \epsilon.,
\]</span></p>
<p>which is clear from Figure 13. This unfortunate result motivates part 3 of <a href="###next-steps">Next Steps</a>.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">Next Steps</h3>
<ol type="1">
<li><p><strong>Apply the Method to Raw, Unfiltered Spike Data</strong><br>
Instead of relying on manual, ad hoc clustering to identify neuronal spike rates, we propose applying CEBRA directly to the raw recorded neural data. This approach could help with issues related to the “curse of neurons,” as it eliminates the dependency on clustering quality and the number of detected neurons.</p></li>
<li><p><strong>Simulate an Online Environment</strong><br>
Test whether this method can be applied in a simulated “online” experimental environment. This would involve decoding <span class="math inline">\(\mathcal{H}\)</span> in real time, allowing closed-loop control of <span class="math inline">\(\mathcal{H}\)</span>.</p></li>
<li><p><strong>Modify the CEBRA Loss Function</strong><br>
Adapt the CEBRA loss function to incorporate constraints that bias the resulting point cloud to lie on a desired topology. For example, in this project we would constrain the point cloud to lie on a 1D ring. This would allow for consistency of the manifolds produced.</p></li>
</ol>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<p>In this work, we introduced a novel manifold learning approach using CEBRA to decode hippocampal gain (<span class="math inline">\(\mathcal{H}\)</span>) without relying on traditional Fourier-based methods. We projected high-dimensional place cell activity into a low-dimensional latent space and validated the resulting circular manifold through persistent homology. Our spline-based “SPUD” parametrization then allowed us to decode hippocampal angle and, consequently, hippocampal gain.</p>
<p>At least 35 well-isolated neurons were required to reliably recover a one-dimensional ring topology. Below this threshold, embeddings frequently collapsed or failed to represent the task structure, leading to inaccurate gain estimates. However, even with at least 35 neurons, accurate recovery of a 1D ring topology was not guaranteed, as the CEBRA loss function does not explicitly enforce this structure.</p>
<p>Despite this challenge, our approach opens new possibilities for using manifold learning to learn neural representations. Looking ahead, integrating raw spike data into the pipeline may alleviate the “curse of neurons” by eliminating the need for explicit clustering of the neuronal data. Adapting these techniques to real-time or “online” experiments could also facilitate neurally closed-loop experiments of this type. Finally, refining the CEBRA loss function to bias embeddings towards intended topological shapes (e.g., 1D ring or toroidal manifolds) could help ensure consistent and accurate results.</p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-chaudhuri2019" class="csl-entry" role="listitem">
Chaudhuri, Rishidev, Berk Gerçek, Biraj Pandey, Adrien Peyrache, and Ila Fiete. 2019. <span>“The Intrinsic Attractor Manifold and Population Dynamics of a Canonical Cognitive Circuit Across Waking and Sleep.”</span> <em>Nature Neuroscience</em> 22: 1512–20. <a href="https://doi.org/10.1038/s41593-019-0460-x">https://doi.org/10.1038/s41593-019-0460-x</a>.
</div>
<div id="ref-jayakumar2019" class="csl-entry" role="listitem">
Jayakumar, Ravikrishnan P., Manu S. Madhav, Francesco Savelli, Hugh T. Blair, Noah J. Cowan, and James J. Knierim. 2019. <span>“Recalibration of Path Integration in Hippocampal Place Cells.”</span> <em>Nature</em> 566 (February): 533–37. <a href="https://doi.org/10.1038/s41586-019-0939-3">https://doi.org/10.1038/s41586-019-0939-3</a>.
</div>
<div id="ref-jin2011" class="csl-entry" role="listitem">
Jin, Xiang, and Jiawei Han. 2011. <span>“K-Medoids Clustering.”</span> In <em>Encyclopedia of Machine Learning</em>, edited by Claude Sammut and Geoffrey I. Webb. Boston, MA: Springer. <a href="https://doi.org/10.1007/978-0-387-30164-8_426">https://doi.org/10.1007/978-0-387-30164-8_426</a>.
</div>
<div id="ref-madhav2022" class="csl-entry" role="listitem">
Madhav, Manu S., Ravikrishnan P. Jayakumar, Shahin G. Lashkari, Francesco Savelli, Hugh T. Blair, James J. Knierim, and Noah J. Cowan. 2022. <span>“The Dome: A Virtual Reality Apparatus for Freely Locomoting Rodents.”</span> <em>Journal of Neuroscience Methods</em>, February. <a href="https://doi.org/10.1016/j.jneumeth.2021.109336">https://doi.org/10.1016/j.jneumeth.2021.109336</a>.
</div>
<div id="ref-madhav2024" class="csl-entry" role="listitem">
Madhav, Manu S., Ravikrishnan P. Jayakumar, Brian Y. Li, Shahin G. Lashkari, Kelly Wright, Francesco Savelli, James J. Knierim, and Noah J. Cowan. 2024. <span>“Control and Recalibration of Path Integration in Place Cells Using Optic Flow.”</span> <em>Nature Neuroscience</em>. <a href="https://doi.org/10.1038/s41593-024-01681-9">https://doi.org/10.1038/s41593-024-01681-9</a>.
</div>
<div id="ref-mitchell-heggs2023" class="csl-entry" role="listitem">
Mitchell-Heggs, Rufus, Seigfred Prado, Giuseppe P. Gava, Mary Ann Go, and Simon R. Schultz. 2023. <span>“Neural Manifold Analysis of Brain Circuit Dynamics in Health and Disease.”</span> <em>Journal of Computational Neuroscience</em> 51: 1–21.
</div>
<div id="ref-moser2008" class="csl-entry" role="listitem">
Moser, Edvard I., Emilio Kropff, and May-Britt Moser. 2008. <span>“Place Cells, Grid Cells, and the Brain’s Spatial Representation System.”</span> <em>Annual Review of Neuroscience</em> 31: 69–89.
</div>
<div id="ref-schneider2023" class="csl-entry" role="listitem">
Schneider, Steffen, Jin Hwa Lee, and Mackenzie Weygandt Mathis. 2023. <span>“Learnable Latent Embeddings for Joint Behavioural and Neural Analysis.”</span> <em>Nature</em> 617: 360–68. <a href="https://doi.org/10.1038/s41586-023-05842-x">https://doi.org/10.1038/s41586-023-05842-x</a>.
</div>
<div id="ref-sebastian2022" class="csl-entry" role="listitem">
Sebastian, Enrique R., Julio Esparza, and Liset M. de la Prida. 2022. <span>“Quantifying the Distribution of Feature Values over Data Represented in Arbitrary Dimensional Spaces.”</span> <em>Institute Cajal, CSIC, Madrid, Spain</em>. <a href="https://doi.org/10.1371/journal.pcbi.1011768.g001">https://doi.org/10.1371/journal.pcbi.1011768.g001</a>.
</div>
<div id="ref-tolman1948" class="csl-entry" role="listitem">
Tolman, Edward C. 1948. <span>“Cognitive Maps in Rats and Men.”</span> <em>Psychological Review</em> 55 (4): 189–208.
</div>
<div id="ref-walker2008" class="csl-entry" role="listitem">
Walker, Brenton. 2008. <span>“Using Persistent Homology to Recover Spatial Information from Encounter Traces.”</span> <em>In Proceedings of the ACM</em>. <a href="https://doi.org/10.1145/1374618.1374668">https://doi.org/10.1145/1374618.1374668</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/bioshape-analysis\.github\.io\/blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>