---
title: "Heterogeneity analysis of cryo-EM data of proteins dynamic in comformation and composition using linear subspace methods"
bibliography: references.bib
date: "September 18 2024" # Format example: August 9 2024
author:
  - name: "Qiyu Wang" 
categories: [cryo-EM] # [biology, bioinformatics, theory, etc.]
---

## Background
Cryogenic electron microscopy (cryo-EM), a cryomicroscopy technique applied on samples embedding in ice, along with recent development of powerful hardwares and softwares, have achieved huge success in the determination of biomolecular structures at near-atomic level. Cryo-EM takes screenshots of thousands or millions of particles in different poses frozen in the sample, and thus allows the reconstruction of the 3D structure from those 2D projections. 

Early algorithms and softwares of processing cryo-EM data focus on resolving homogeneous structure of biomolecules. However, many biomolecules are very dynamic in conformations, compositions, or both. For example, ribosomes comprise of many sub-units, and their compositions may vary within the sample and are of research interest. Spike protein is an example of conformational heterogeneity, where the receptor-binding domain (RBD) keeps switching between close and open states in order to bind to receptors and meanwhile resist the binding of antibody. When studying the antigen-antibody complex, both compositional and conformational heterogeneity need to be considered.

![A simple illustration of the conformational heterogeneity of spike protein, where it displays two kinds of conformations: closed RBD and open RBD of one chain (colored in blue) [@Wang2020]. Spike protein is a trimer so in reality all the three chains will move possibly in different ways and the motion of spike protein is much more complex than what's shown here.](img/spike.png){ width=65% style="display: block; margin-left: auto; margin-right: auto;" }

The initial heterogeneity analysis of 3D structrues reconstructed from cryo-EM data started from relatively simple 3D classfication, which outputs discrete classes of different conformations. This is usually done by expectation-maximization (EM) algorithms, where 2D particle stacks were iteratively assigned to classes and used to reconstruct the volume of that class. However, such an approach has two problems: first, the classification decreases the number of images used to reconstruct the volume, and thus lower the resolution we are able to achieve; second, the motion of biomolecule is continuous in reality and discrete classification may not describe the heterogeneity very well, and we may miss some transient states.

Therefore, nowadays people start to focus on methods modeling continuous heterogeneity without any classification step to avoid the above issues. Most methods adopt similar structures, where 2D particle stacks are mapped to latent embeddings, clusters/trajectories are estimated in latent space, and finally volumes are mapped and reconstructed from latent embeddings. Early methods use linear mapping (e.g. 3DVA), but with the applications of deep learning techniques in the field of cryo-EM data processing, people find methods adapted from variational autoencoder (VAE) achieving better performance (e.g. cryoDRGN, 3DFlex). Nevertheless, the latent space obtained from VAE and other deep learning methods is hard to interpret, and do not conserve distances and densities, imposing difficulties in reconstructing motions/trajectories, which are what most structure biologists desire at the end.

Recent developed software RECOVAR [@Gilles2023], using a linear mapping like 3DVA, was shown to achieve comparable or even better performance with deep learning methods, and meanwhile has high interpretability and allows easy recovery of motions/trajectories from latent space. For this project, I will review the pipeline of RECOVAR, discussed improvements and extensions we made to this pipeline, and present heterogeneity analysis results from the original paper and our SARS-CoV2 spike protein dataset.

## Methods

### Regularized covariance estimation
Let $N$ be the dimension of the grid and $n$ be the number of images. We start with formulating the formation process of each cryo-EM image in the Fourier space $y_i\in\mathbb{C}^{N^2}$ from its corresponding conformation $x_i\in\mathbb{C}^{N^3}$ as:
$$y_i = C_i\hat{P}(\phi_i)x_i + \epsilon_i, \epsilon_i\sim N(0, \Lambda_i) $$

where $\hat{P}(\phi_i)$ is the projetion from 3D to 2D after rigid body motion with pose $\phi_i$, $C_i$ is the contrast transfer function (CTF), and $\epsilon_i$ represents the Gaussian noise. RECOVAR will assume that $C_i$ and $\phi_i$ were given. This can be done via many existing ab-initio methods. Hence in the following analysis, we will simply fix the linear map $P_i:=C_i\hat{P}(\phi_i)$.

When poses are known, the mean $\mu\in\mathbb{C}^{N^3}$ of the distribution of conformations can be estimated by solving:

$$\hat{\mu}:=\underset{\mu}{\mathrm{argmin}}\sum_{i=1}^{n}\lVert y_i-P_i\mu\rVert_{\Lambda^{-1}}^2+\lVert\mu\rVert_w^2$$

where $\lVert v\rVert_{\Lambda^{-1}}^2=v^*\Lambda^{-1}v$ and $\lVert v\rVert_w^2=\sum_i|v_i|^2w_i$. $w\in \mathbb{R}^{N^3}$ is the optional Wiener filter. Similarly, covariance can be estimated as the solution to the linear system corresponding to the following:

$$\hat{\Sigma}:=\underset{\Sigma}{\mathrm{argmin}}\sum_{i=1}^n\lVert(y_i-P_i\hat{\mu})(y_i-P_i\hat{\mu})^*-(P_i\Sigma P_i^*+\Lambda_i)\rVert_F^2+\lVert\Sigma\rVert_R^2$$

where $\lVert A\rVert_F^2=\sum_{i,j}A_{i,j}^2$ and $\lVert A\rVert_R^2=\sum_{i,j}A_{i,j}^2R_{i,j}$. $R$ is the regularization weight.

Our goal at this step is to compute principal components (PCs) from $\hat{\mu}$ and $\hat{\Sigma}$. Nevertheless, computing the entire matrix of $\hat{\Sigma}$ is impossible considering that we have to compute $N^6$ entries. Fortunately, for low-rank variance matrix only a subset of the columns is required to estimate the entire matrix and its leading eigenvectors, which are just PCs. $d$ PCs can be computed in $O(d(N^3+nN^2))$, much faster than $O(N^6)$ required to compute the entire covariance matrix. Here a heuristic scheme is used to choose which columes to be used to compute eigenvectors. First, all columns are added into the considered set. Then the column corresponding to the pixel with the highest SNR in the considered set is iteratively added to the chosen set, and pixels nearby are removed from the considered set, until there are a disirable number of columns $d$ in the chosen set. We estimate the entries  of the chosen columns and their complex conjugates and let them form $\hat{\Sigma}_{col}$. Let $\tilde{U}\in\mathbb{C}^{N^3\times d}$ be orthogonalized $\hat{\Sigma}_{col}$. It follows that we can compute the reduced covariance matrix $\hat{\Sigma}_{\tilde{U}}$ by:

$$\hat{\Sigma}_{\tilde{U}}:=\underset{\Sigma_{\tilde{U}}}{\mathrm{argmin}}\sum_{i=1}^n\lVert(y_i-P_i\hat{\mu})(y_i-P_i\hat{\mu})^*-(P_i\tilde{U}\Sigma_{\tilde{U}}\tilde{U}^* P_i^*+\Lambda_i)\rVert_F^2$$

Basically, we just replace $\Sigma$ in the formula to estimate the entire covariance matrix shown before with $\tilde{U}\Sigma_{\tilde{U}}\tilde{U}^*$. Finally, we just need to perform an eigendecomposition on $\hat{\Sigma}_{\tilde{U}}$ and obtain $\hat{\Sigma}_{\tilde{U}}=V\Gamma V^*$. The eigenvectors (which are the PCs we want) and eigenvalues would be $U:=\tilde{U}V$ and $\Gamma$ repectively.

### Latent space embedding
With PCs computed from the last step, denoted by $U\in\mathbb{C}^{N^3\times d}$, we can project $x_i$ onto lower-dimensional latent space by $z_i = U^*(x_i-\hat{\mu})\in\mathbb{R}^d$. Assuming $z_i\sim N(0,\Gamma)$, the MAP estimation of $P(z_i|y_i)$ can be obtained by solving: 

$$\hat{a}_i, \hat{z}_i = \underset{a_i\in\mathbb{R}^+, z_i\in\mathbb{R}^d}{\mathrm{argmin}}\lVert a_iP_i(Uz_i+\hat{\mu})-y_i\rVert_{\Lambda_i^{-1}}^2+\lVert z_i\rVert_{\Gamma^{-1}}^2$$

where $a_i$ is a scaling factor used to capture the effect of display variations in contrast.

### Conformation reconstruction
After computing the latent embeddings, the next question would naturally be how to reconstruct conformations from embeddings. The most intuitive way is to do reprojection *i.e*. $\hat{x}\leftarrow Uz+\hat{\mu}$. Nevertheless, reprojection only works well when all the relevant PCs can be computed, which is almost impossible considering the low signal-to-noise ratio (SNR) in practice. Therefore, an alternative scheme based on adaptive kernel regression is used here. Given a fixed latent position $z^*$ and the frequency $\xi^k\in\mathbb{R}^3$ in the 3D Fourier space of the volume whose value we would like to estimate, the kernel regression estimates of this form are computed as:

$$x(h;\xi^k) = \underset{x_k}{\mathrm{argmin}}\sum_{i,j}\frac{1}{\sigma_{i,j}^2}|C_{i,j}x_k-y_{i,j}|^2K(\xi^k,\xi_{i,j})K_i^h(z^*,z_i)$$

where $h$ is bandwitdth; $\sigma_{i,j}$ is the variance of $\epsilon_{i,j}$, which is the noise of frequency $j$ of the $i$-th observation; $y_{i,j}$ is the value of frequency $j$ of the $i$-th observation; $\xi_{i,j}$ is the frequency $j$ of the $i$-th observation in 3D adjusted by $\phi_i$. We have two kernel functions in this formulation. $K(\xi^k,\xi_{i,j})$
is the triangular kernel, measuring the distance in frequencies. $K_i^h(z^*, z_i)=E(\frac{1}{h}\lVert z^* - z_i\rVert_{\Sigma_{z_i}^{-1}})$ where $\Sigma_{z_i}$ is the covariance matrix of $z_i$ which can be computed from the formulation for latent embedding, and $E$ is a piecewise constant approxination of the Epanechnikov kernel. $K_i^h(z^*, z_i)$ measures the distance between latent embeddings.

Here comes a trade-off at the heart of every heterogeneous reconstruction algorithm: averaging images is necessary to overcome noise, but it also degrades heterogeneity since the images averaged may come from different conformations. Hence, we have to choose $h$ carefully. A cross-validation strategy is applied to find the optimal $h$ for each frequency shell of each subvolume. For a given $z^*$, the dataset is split into two: from one halfset, the 50 estimates $\hat{x}(h_1), ..., \hat{x}(h_{50})$ with varying $h$ are computed, and from the other subset a single low-bias, high-variance template $\hat{x}_{CV}$ is reconstrcuted by using a small number of images which are closest to $z^*$. Each of the 50 kernel estimate is then subdivided into small subvolumes by real-space masking, and each subvolume is again decomposed into frequency shells after a Fourier transform. We use the following cross-validation metric for subvolume $v$ and frequency shell $s$:

$$d_{s,v}(h) = \lVert S_sV^{-1/2}(M_v(\hat{x}_{CV}-\hat{x}(h)))\rVert_2^2$$

where $S_s$ is a matrix that extracts shell $s$; $M_v$ is a matrix extracting subvolume $v$; and $V$ is a diagonal matrix containing the variance of the template. For each $s$ and $v$, the minimizer over $h$ of the cross-validarion score is selected, and the final volume is obtained by first recombining frequency shells for each subvolume and then recombining all the subvolumes.

![Volumes are reconstructed from the embedding by adaptive kernel regression.](img/3D_reconstruct.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

### Estimation of state density
Since motion is what structure biologists finally want, we have to figure out a method to sample from latent space to form a trajectory representing the motion of the molecule. According to Boltzmann statistics, the density of a particular state is a measure of the free energy of that state, which means a path which maximizes conformational density is equivalent to the path minimizing the free energy. Taking the advantage of linear mapping, we can easily relate embedding density to conformational density. The embedding density estimator is given by:

$$\hat{E}(z) = \frac{1}{n}\sum_{i=1}^nK_G(\hat{z_i}, \Sigma_s;z)$$

where $K_G(\mu, \Sigma;z)$ is the probability density function of the multivariant Gaussian with mean $\mu$ and covariance $\Sigma$, evaluated at $z$, and $\Sigma_s$ is set using the Silverman rule. The conformational density can be related as following:

 $$\overline{E}(z)=\overline{G}(z)*d(z)$$

 where $\overline{E}(z)$ is the expectation of the embedding density $\hat{E}(z)$; $\overline{G}(z)$ is the expectation of $\hat{G}(z)=\frac{1}{n}\sum_{i=1}^nK_G(0,\Sigma_{z_i}+\Sigma_s;z)$, which is named as embedding uncertainty; $d(z)$ is the conformational density corresponding to $z$; $*$ is the convolution operation.

### Motion recovery
Given the conformational density estimated from last step, denoted by $\hat{d}(z)$, start state $z_{st}$ and end state $z_{end}$, we can find trajectory $Z(t):\mathbb{R}^+\rightarrow\mathbb{R}^d$ in latent space by computing the value function:

$$v(z):=\underset{Z(t)}{\mathrm{inf}}\int_{t=0}^{t=T_a}\hat{d}(Z(t))^{-1}dt$$

subject to
$$Z(0)=z, Z(T_a)=z_{end}, \lVert \frac{d}{dt}Z(t)\rVert=1; T_a = min\{t|Z(t)=z_{end}\}$$

In simple word, $v(z)$ computes the minimum inverse density we can have to reach $z_{end}$ starting from $z$. $v(z)$ is the viscosity solution of the Eikonal equation:

$$\hat{d}(z)|\nabla v(z)|=1, \forall z\in B\setminus \{z_{end}\}; v(z_{end})=0$$

where $B$ is the domain of interest, and $v(z)$ can be solved by solving this partial differential equation. Once $v(z)$ is solved, the optimal trajectory an be obtained by finding the path orthogonal to the level curve of $v(z)$, which can be computed numerically using the steepest gradient descent on $v(z)$ starting from $z_{st}$

![Visulization of the steepest gradient descent on the level curve of v(z)](img/level_curve.png){ width=45% style="display: block; margin-left: auto; margin-right: auto;" }

### Extentions to RECOVAR: MPPC for path discovery
The density-based path discovery algorithm used by RECOVAR is based on physical considerations that molecules prefer to take the path with lowest free energy, which is the path with highest density in the latent space, and is rebust against outliers. Nevertheless, the time to deconvolve density is exponential of the number of PCs, and deconvolution requires large memory. Our 24GB GPU can deconvolve density at most a dimension of 4, but 4 PCs are usually not enough to capture enough heterogeneity as shown in the figure below, which shows how the eigenvalues change with the number of PCs when applying RECOVAR to the SARS-CoV2 spike dataset. There are still quite large drops in the eigenvalue after 4 PCs.

![Eigenvalues vs. the number PCs of the SARS-CoV2 spike dataset applied with RECOVAR](img/eigenvalue.png){ width=45% style="display: block; margin-left: auto; margin-right: auto;" }

Therefore, we proposed an alternative method to discover path by computing multiple penalized principal curves (MPPC) [@slav2017]. The basic idea of MPPC is to find one or multiple curves to fit all the given points as close as possible, with constraints in the number and lengths of the curves. In order to be solved numerically, the curves are usually discretized. Let $y^1 = (y_1, y_2, ..., y_{m_1}), y^2 = (y_{m_1+1}, y_{m_1+2}, ..., y_{m_1+m_2}),...,y^k = (y_{m-m_k+1}, y_{m-m_k+2},...,y_{m_k})$ to be $k$ curves represented by points. Let $s_c = \sum_{j=1}^{c}m_j$ be the indices of the end points of curve $c$. Each point $x_i$ in the data to fit is assigned to the closest point on the curves, and we denote $I_j$ to be the group of indices of data points that are assigned to curve point $y_j$. The goal is to minimize:
$$\sum_{j=1}^m\sum_{i\in I_j}w_i|x_i-y_j|^2+\lambda_1\sum_{c=0}^{k-1}\sum_{j=1}^{m_c+1}|y_{s_c+j+1}-y_{s_c+j}|+\lambda_1 \lambda_2 (k-1)$$

$\sum_{j=1}^m\sum_{i\in I_j}w_i|x_i-y_j|^2$ penalizes the distance of the curves to data points, $\lambda_1\sum_{c=0}^{k-1}\sum_{j=1}^{m_c+1}|y_{s_c+j+1}-y_{s_c+j}|$ is the total length of all the curves, and $\lambda_1 \lambda_2 (k-1)$ controls the number of curves. Applied to our case, we set $w_i$ to be the inverse of the trace of the covariance matrix of the embedding to make the curves fit better to those embeddings with high confidence.

### Extensions to RECOVAR: atomic model fitting
When resolving homogenoeous structures of proteins, atomic models are usually the final product instead of density maps as they contain more structural information. Atomic models are fitted into density maps either manually or automatically, but most start from scratch, which is very inefficient to be applied to density map series because the difference between the neighboring maps should be relatively small. We can take advantage of this property by updating coordinates of the fitted model of the previous frame to get the model fitted in the current density map. Hence, we proposed two algorithms to fit atomic model, both are based on gradient descent.

Let $R_{t-1}\in \mathbb{R}^{N_a\times 3}$ be the fitted atomic model of the $t-1$th density map, where $N_a$ is the number of atoms in the protein. We can use deposited protein structure or model predicted from sequence using algorithms like AlphaFold as $R_1$. Let $V_t\in\mathbb{R}^{N\times N\times N}$ be the $t$th density map we want to fit in, where $N$ is the grid size. We cannot directly minimize the "distance" between $R_{t-1}$ and $V_t$, because atomic coordinates cannot be compared with volume maps. A natural way to solve this issue is to map atomic coordinates to density map with a function $f: \mathbb{R}^{N_a\times 3}\rightarrow \mathbb{R}^{N\times N\times N}$, for example, by summing up the gaussian kernels centered at each coordinate i.e.:
$$V_t({\bf r}=(x,y,z)^T) = \sum_{k=1}^{N_a} \exp -\frac{\|{\bf r} - R_t[k]\|_2^2}{2\sigma_k^2}$$

However, the computational time for one mapping is $O(N^3N_a)$, which is already very slow, even without considering the fact that we have to map coordinates to densities in many update iterations. Hence in practice we used truncated gaussian kernels.

Now we have all the tools needed to have algorithms fit atomic model $R_{t-1}$ into density map $V_t$. Our first algorithm purely based on gradient descent will be:

![](img/algorithm1.png){ width=120% style="display: block; margin-left: auto; margin-right: auto;" }

When computing the loss used for gradient descent, we included not only the difference between $V_t$ and mapped density from coordinates, but also the difference between the starting and current bond lengths/angles to preserve the original structure. In practice, we computed intra-residue bond lengths i.e. bond lengths of $N-CA, CA-C \text{ and } C-O$ and inter-residue bond lengths $C_i-N_{i+1}$. In practice proteins can have multiple chains (like SARS-CoV2 spike which has three chains), so we set the inter-residue bond lengths at end points of the chains to be $0$. We used dihedral angles $\phi$ (i.e. angles formed by $C_i-N_{i+1}-CA_{i+1}-C_{i+1}$) and $\psi$ (i.e. angles formed by $N_i-CA_i-C_i-N_{i+1}$) as bond angles, and similarly set the dihedral angles cross the chains to be $0$.

One weakness of gradient descent is that it can be easily stuck in local optima. Leveraging the recent progress in diffusion models for protein generation, we proposed the second algorithm as following:

![](img/algorithm2.png){ width=120% style="display: block; margin-left: auto; margin-right: auto;" }

The inner for loop is the same as Algorithm1, where the coordinates are updated through gradient descent. The difference is that an outer loop is added which diffuses and then denoises the fitted coordinates from previous round of gradient descent and used the denoised coordinates as the starting model of the current round of gradient descent. We adapted the diffusion model and graph neural network (GNN) based denoiser from Chroma [@ingraham2023].


## Results

### Results of public datasets
The original paper of RECOVAR presents results on precatalytic spliceosome dataset (EMPIAR-10180), integrin dataset (EMPIAR-10345) and ribosomal subunit dataset (EMPIAR-10076), all of which are public dataset and could be accessed from https://www.ebi.ac.uk/empiar/. 

Results on EMPIAR-10180 focuses on comformational heterogeneity. Three local maxima in conformational density were identified, a path between two of which was identified to show arm regions moving down followed by head regions moving up.

![Latent space and volume view of precatalytic spliceosome conformational heterogeneity. Latent view of the path is projected on the plane formed by different combinations of two principal components.](img/EMPIAR-10180.png){ width=75% style="display: block; margin-left: auto; margin-right: auto;" }

EMPIAR-10345 contains both conformational and compositional heterogeneity. Two local maxima were found, with the smaller one corresponds to a different composition never reported by provious studies. Also a motion of the arm was found along the path.

![RECOVAR finds both comformational and compositional heterogeneity within integrin](img/EMPIAR-10345.png){ width=45% style="display: block; margin-left: auto; margin-right: auto;" }

EMPIAR-10076 is used to show the ability of RECOVAR to find stable states. RECOVAR finds two stable states of the 70S ribosomes.

![The volume of two stable states are reconstructed, correspinding to two peaks in densities](img/EMPIAR-10076.png){ width=65% style="display: block; margin-left: auto; margin-right: auto;" }

### Results of SARS-CoV2 datasets

We also tested RECOVAR on our own dataset which contains 271,448 SARS-CoV2 spike protein particles, extracted using CryoSparc. Some of these particles are bounded to human angiotensin-converting enzyme 2 (ACE2), which is an enzyme on human membrane targeted by SARS-CoV2 spike protein. Therefore, this dataset has both compositional and conformational heterogeneity.

After obtaining an ab-initio model from CryoSparc, we ran RECOVAR with a dimension of 4 and a relatively small grid size of 128. K-Means clustering was performed to find 5 cluster centers among the embeddings. 

Here we present two volumes reconstructed from center 0 and center 1, showing a very obvious compositional heterogeneity, where ACE2 is clearly present in center 0 and missing in center 1.

![Compositional heterogeneity in the spike protein dataset. The spot where ACE2 is present/absent is highlighted by the red circle.](img/composition_spike.png){ width=65% style="display: block; margin-left: auto; margin-right: auto;" }

A path between center 0 and 1 was analyzed to study the conformational changes adopted by the spike protein to bind to ACE2. We can see the arm in the RBD region lifts in order to bind to ACE2.

![Conformational changes along the path between center 0 and 1, highlighted by the yellow circle](img/conformation_spike.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

Below is the complete movie between state 0 and 1:
{{< video img/movie5.mp4>}}

### Comparison of paths discovered by density vs. MPPC
To compare the paths generated by MPPC to original density-based approaches, we ran MPPC on the embedings with a dimension of 4. The figure below shows the path generated by density-based methods from state 0 to 1 and 2 to 3, and path output by MPPC:

![Paths in 4D space output by density-based methods and MPPC](img/path_compare_4d.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

We can see that path between 0 and 1 is completely missing in MPPC path. Path from 2 to 3 presents in MPPC path between the orange node and purple node, but is slightly pulled towards outliers.

It is mentioned in Methods that one advantage to use MPPC is that its low computational cost allows us to fit data in higher dimension, so we also fit MPPC to data in 10D. The results are present in the figure below:

{{< video img/movie4.mp4>}}

We can see that the spike in the 10D movie is more flexible and there are more changes in the shape than 4D.

### Results of atomic model fitting
We first tested two atomic model fitting algorithms on most simple case where we started from an atomic model and fit into a density map that is close to the starting model. We took deposited SARS-CoV2 spike protein structure 7V7R as initial model and generated target density map with truncated gaussian kernels from another protein 7V7Q.

![Starting (7V7R in blue) and target (7V7Q in brown) SARS-CoV2 spike used to test atomic model fitting algorithms](img/starting_model.png){ width=40% style="display: block; margin-left: auto; margin-right: auto;" }

We ran 12000 iterations for Algorithm1. To make a fair comparison, same number of total loops, composed of 60 outer diffusion loops and 200 inner gradient descent loops were run with Algorithm2. We kept the regularization parameters the same for both algorithms. Both algorithms took about 950 seconds to complete. In UCSF Chimera [@pettersen2004], we aligned initial model 7V7R, fitted model from Algorithm1 and fitted model output by Algorithm2 with 7V7Q, computed root mean square deviation (RMSD) between aligned coordinates, and annotated the structures with red-blue color where red denotes high RMSD (large difference) and blue means low RMSD (small difference). The results are shown below:

![Left: initial model (7V7R) aligned with target model (7V7Q); Middle: fitted model from Algorithm1 aligned with target model (7V7Q); Right: fitted model from Algorithm2 aligned with target model (7V7Q)](img/fit_result.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

Surprisingly, Algorithm1 performs better than Algorithm2, with more regions in deep blue color indicating low RMSD, though its design is relatively simple. Overall, both algorithms make significant progress from the initial model to fit into the target density map.

To test whether there will be a significant accumulation of the errors if we kept using fitted model from last frame as the initial model to fit the current frame, we used our algorithms to fit into a series of three density maps, generated from protein 7V7R, 7V7Q, and 7V7S, starting from 7V7P.
Consecutive proteins in the series were aligned and local RMSD was computed to visualize the degree of conformational changes at different regions of different frames more intuitively.

![Starting from 7V7P (brown), we fit the density map simulated from 7V7R (pink), 7V7Q (blue), and 7V7S (green) sequentially](img/series.png){ width=40% style="display: block; margin-left: auto; margin-right: auto;" }

![Aligment of consecutive proteins in the sereis for test](img/series_rmsd.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

Most conformational changes in this series occur in the RBD region, with 7V7S undergoing the most significant changes, and expected to be the hardest model to fit. We used the same parameters as before to fit each model with both algorithms, and followed the same procedure to compute and visualize local RMSD for each frame in the series.

![Test results on series from Algorithm1](img/series_simple.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

![Test results on series from Algorithm2](img/series_gnn.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

Same as the previous test, Algorithm1 performs better than Algorithm2 in fitting all the maps in the series. Compared with fitting to maps generated from 7V7Q starting with "true" 7V7R, initializing model with fitted 7V7R from previous step does not lead to siginificant increase in RMSD in fitted 7V7Q. There are some white regions with medium RMSD shared by three fitted models, but the RMSD of these regions does not increase. There is a part with high RMSD in the left region of the last structure 7V7S in the series, but it seems that the error is not accumulated from previous fitting as the RMSD of this region of the privious fitting is very low. 


## Discussion
RECOVAR has several advantages over other heteogeneity analysis methods. Besides the high interpretability we mentioned before, RECOVAR is proved to be able to discover compositional heterogeneity, which cannot be solved by some popular deep learning methods like 3DFlex. Moreover, RECOVAR has much less hyper-parameters to tune compared with deep learning models. The main hyper-parameter the user needs to specify is the number of proncipal components to use, which is a trade-off between the amount of heterogeneity to capture and computational cost.

However, one problem RECOVAR and many other heterogeneity analysis algorithms share is that it requires the input of a homogeneous model/poses of images. However the estimation of the consensus model is often biased by heterogeneity, while the heterogeneity analysis assumes the input consensus model is correct(a dead loop!). Nevertheless, we would expect this issue to be solved by an EM-algorithm iteratively constructing consensus model and performing heterogeneity analysis. In future we may also be interested in benchmarking on pose estimation errors, and other parameters such as the number of principal components, grid size, and particle number, which were not be done in the original paper.

Another drawback of RECOVAR is that the density-based path discovery approach is computationally expensive. In this project we proposed MPPC as an alternative approach to compute path. Although this a method can be used to find paths in higher dimension with very fast speed, it is more sensitive to outliers. One way to address is this issue is to iteratively remove points that are far away from the curves and then fit the curve. Another feature of MPPC is that it does not take the starting and ending points. This can be either an advantage or disadvantage, depending on the objective. MPPC works if the goal is to study the conformational change trajectory in the entire space. Nevertheless, if we are more interested in how proteins transit between two specific states, MPPC may output path even not passing these two states.

Moreover, the path output contains both conformational and compositional heterogeneity. From the movies of the spike we can see ACE2 suddenly appear or disappear at the top of the lifted RBD region. This will cause trouble to atomic fitting. In the conventional pipeline, people address this problem via discrete 3D classification to separate particles with different compositions, which may not have very high accuracy when applied to complex datasets with both compositional and comformational heterogeneity. Actually 3D classification of cryoSPARC fails to distinguish particles with and without ACE2 on our spike protein dataset without templates. Here instead we may want to leverage the powerful tool of RECOVAR, and directly classify particles in the continuous latent space. One potential approach would be segment latent space based on the mass of the volume associated with the embeddings. This approach may not work in the case where the compositional difference does not lead to a change in mass, but as long as the compositional heterogeneity leads to difference in mass that is more significant than noise (like SARS-CoV2 spike + ACE2 in our case), this method should work. We checked the feasibility of this approach by computing mass of the density maps along time in a movie output by RECOVAR using our SARS-CoV2 data as following:

![Illustrations of how mass of the density map changes in the movie of SARS-CoV2 spike, some bounded to ACE2](img/mass.png){ width=100% style="display: block; margin-left: auto; margin-right: auto;" }

This movie demonstrates a relatively complex changes in spike proteins, where the spike undergoes the following changes: one RBD up + one ACE2 -> one RBD up -> all RBDs down -> 1 RBD up -> 2 RBDs up + 1 ACE2 -> 2 RBDs up + 2 ACE2's. There is a clear cutoff at mass of around 900,000 above which ACE2 is present. The difference in the mass between those with 1 ACE2 and 2 ACE2's are not very obvious, but separating spike with and without ACE2 is enough for the purpose of atomic model fitting to the density maps from closed states up to the moment where the RBD completely lifts but without ACE2.

Regarding to our atomic model fitting algorithms, Algorithm1 which is purely based on gradient descent works surprisingly well. Although some regions with medium quality of fitting in the first frame are inherited by later fittings, the RMSD does not rise further. One improvement we can make to our current algorithms is to change the constant sigma in the gaussian kernel to map coordinates to density maps to an annealing parameter. Initially we make sigma large to enable the model to undergo large conformational changes. Later we shrink the size of sigma for better fitting in the local region.