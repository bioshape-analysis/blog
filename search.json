[
  {
    "objectID": "posts/quasiconformalmap/index.html#theorem",
    "href": "posts/quasiconformalmap/index.html#theorem",
    "title": "Quasiconformal mapping in 1D",
    "section": "Theorem",
    "text": "Theorem"
  },
  {
    "objectID": "posts/AFM-data/index.html",
    "href": "posts/AFM-data/index.html",
    "title": "Extracting cell geometry from AFM",
    "section": "",
    "text": "We present here the protocole to process biological images such as bacteria atomic force miroscopy data. We want to study the bacteria cell shape and extract the main geometrical feature."
  },
  {
    "objectID": "posts/AFM-data/index.html#biological-context",
    "href": "posts/AFM-data/index.html#biological-context",
    "title": "Extracting cell geometry from AFM",
    "section": "Biological context",
    "text": "Biological context\nMycobacterium smegmatis is Grahm-positive rod shape bacterium. It is 3 to 5 \\(\\mu m\\) long and around 500 \\(nm\\) wide. This non-pathogenic species is otften used a biological model to study the pathogenic Mycobacteria such as M.tuberculosis (responsible for the tubercuosis) or M.abscessus, with which it shares the same cell wall structure(Tyagi and Sharma 2002) . In particular M.smegmatis has a fast growth (3-4 hours doubling time compared to 24h for M. tuberculosis), allowing for faster experimental protocols.\nHere are some know properties of M.smegmatis bacteria :\n\nThey present variation of cell diameter along their longitudinal axis (Eskandarian et al. 2017). The cell diameter is represented as a height profile along the cell centerline. We respectively name peaks and troughs the local maxima and minima of this proile.\n\n\n\n\n3D image of M.smegmatis. The orange line represents the height profile.\n\n\n\nThey grow following a biphasic and asymetrical polar dynamics(Hannebelle et al. 2020). The cells elongate from the poles, where material is added. After division, the pre-existing pole (OP) elongate at a high rate, whereas the newly created pole (NP) has first a slow growth, and then switches to a fast growth, after the New End Take Off (NETO).\n\n\n\n\nGrowth dynamics."
  },
  {
    "objectID": "posts/AFM-data/index.html#raw-image-pre-processing",
    "href": "posts/AFM-data/index.html#raw-image-pre-processing",
    "title": "Extracting cell geometry from AFM",
    "section": "Raw image pre-processing",
    "text": "Raw image pre-processing\n\nData\nSeveral data acquisitions were conducted with wild types and different mutant strains. The raw data is composed of AFM log files times series for each experiments. Each log file contain several images, each one representing a physical channel such as height, stiffness, adhesion etc."
  },
  {
    "objectID": "posts/AFM-data/index.html#segmentation",
    "href": "posts/AFM-data/index.html#segmentation",
    "title": "Extracting cell geometry from AFM",
    "section": "Segmentation",
    "text": "Segmentation"
  },
  {
    "objectID": "posts/AlphaShape/index.html",
    "href": "posts/AlphaShape/index.html",
    "title": "Alpha Shapes in 2D and 3D",
    "section": "",
    "text": "Alpha shapes are a generalization of the convex hull used in computational geometry. They are particularly useful for understanding the shape of a point cloud in both 2D and 3D spaces. In this document, we will explore alpha shapes in both dimensions using Python."
  },
  {
    "objectID": "posts/AlphaShape/index.html#introduction",
    "href": "posts/AlphaShape/index.html#introduction",
    "title": "Alpha Shapes in 2D and 3D",
    "section": "",
    "text": "Alpha shapes are a generalization of the convex hull used in computational geometry. They are particularly useful for understanding the shape of a point cloud in both 2D and 3D spaces. In this document, we will explore alpha shapes in both dimensions using Python."
  },
  {
    "objectID": "posts/AlphaShape/index.html#d-alpha-shape",
    "href": "posts/AlphaShape/index.html#d-alpha-shape",
    "title": "Alpha Shapes in 2D and 3D",
    "section": "2D Alpha Shape",
    "text": "2D Alpha Shape\nTo illustrate alpha shapes in 2D, we’ll use the alphashape library. Let’s start by generating a set of random points and compute their alpha shape.\nFirst we create point cloud points:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport alphashape\nfrom matplotlib.path import Path\nfrom scipy.spatial import ConvexHull\n\ndef generate_flower_shape(num_petals, num_points_per_petal):\n    angles = np.linspace(0, 2 * np.pi, num_points_per_petal, endpoint=False)\n    r = 1 + 0.5 * np.sin(num_petals * angles)\n    \n    x = r* np.cos(angles)\n    \n    y = r * np.sin(angles)\n    \n    return np.column_stack((x, y))\n\ndef generate_random_points_within_polygon(polygon, num_points):\n    \"\"\"Generate random points inside a given polygon.\"\"\"\n    min_x, max_x = polygon[:, 0].min(), polygon[:, 0].max()\n    min_y, max_y = polygon[:, 1].min(), polygon[:, 1].max()\n    \n    points = []\n    while len(points) &lt; num_points:\n        x = np.random.uniform(min_x, max_x)\n        y = np.random.uniform(min_y, max_y)\n        if Path(polygon).contains_point((x, y)):\n            points.append((x, y))\n    \n    return np.array(points)\n\nplt.figure(figsize=(8, 6))\npoints = generate_flower_shape(num_petals=6, num_points_per_petal=100)\npoints = generate_random_points_within_polygon(points, 1000)\nplt.scatter(points[:, 0], points[:, 1], s=10, color='blue', label='Points')\n\n/Users/wenjunzhao/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning:\n\nA NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.3\n\n\n\n\n\n\n\n\n\n\nTry run this with alpha shape radius 0.1:\n\n# Create alpha shape\nalpha = 0.1\nalpha_shape = alphashape.alphashape(points, alpha)\n\n# Plot points and alpha shape\nplt.figure(figsize=(8, 6))\nplt.scatter(points[:, 0], points[:, 1], s=10, color='blue', label='Points')\nplt.plot(*alpha_shape.exterior.xy, color='red', lw=2, label='Alpha Shape')\nplt.title('2D Alpha Shape')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nOops, it seems the radius we picked is too big! Let’s try a few other choices.\n\nalpha_values = [0.1, 5.0, 10.0, 15.0]\n# Plot the flower shape and alpha shapes with varying alpha values\nfig, axes = plt.subplots(2, 2, figsize=(6,6))\naxes = axes.flatten()\n\nfor i, alpha in enumerate(alpha_values):\n    # Compute alpha shape\n    alpha_shape = alphashape.alphashape(points, alpha)\n    \n    # Plot the points and the alpha shape\n    ax = axes[i]\n    #print(alpha_shape.type)\n    if alpha_shape.type == 'Polygon':\n        ax.plot(*alpha_shape.exterior.xy, color='red', lw=2, label='Alpha Shape')\n    ax.scatter(points[:, 0], points[:, 1], color='orange', s=10, label='Point Cloud')\n    \n    \n    \n    ax.set_title(f'Alpha Shape with alpha={alpha}')\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n/var/folders/k7/s0t_zwg11h56xb5xp339s5pm0000gp/T/ipykernel_84480/885549844.py:13: ShapelyDeprecationWarning:\n\nThe 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead."
  },
  {
    "objectID": "posts/sy mds tunnel/index.html",
    "href": "posts/sy mds tunnel/index.html",
    "title": "Multi Dimensional Scaling of ribosome exit tunnel shapes",
    "section": "",
    "text": "The ribosome exit tunnel is a sub-compartment of the ribosome whose geometry varies significantly across species, potentially affecting the translational dynamics and co-translational folding of nascent polypeptide1.\nAs the recent advances in imaging technologies result in a surge of high-resolution ribosome structures, we are now able to study the tunnel geometric heterogeneity comprehensively across three domains of life: bacteria, archaea and eukaryotes.\nHere, we present some methods for large-scale analysis and comparison of tunnel structures."
  },
  {
    "objectID": "posts/sy mds tunnel/index.html#summary-and-background",
    "href": "posts/sy mds tunnel/index.html#summary-and-background",
    "title": "Multi Dimensional Scaling of ribosome exit tunnel shapes",
    "section": "",
    "text": "The ribosome exit tunnel is a sub-compartment of the ribosome whose geometry varies significantly across species, potentially affecting the translational dynamics and co-translational folding of nascent polypeptide1.\nAs the recent advances in imaging technologies result in a surge of high-resolution ribosome structures, we are now able to study the tunnel geometric heterogeneity comprehensively across three domains of life: bacteria, archaea and eukaryotes.\nHere, we present some methods for large-scale analysis and comparison of tunnel structures."
  },
  {
    "objectID": "posts/sy mds tunnel/index.html#tunnel-shape",
    "href": "posts/sy mds tunnel/index.html#tunnel-shape",
    "title": "Multi Dimensional Scaling of ribosome exit tunnel shapes",
    "section": "Tunnel Shape",
    "text": "Tunnel Shape\nThe ribosome exit tunnel spans from the peptidyl-transferase center (PTC), where amino acids are polymerized onto the growing nascent chain, to the surface of the ribosome.\nTypically, it measures 80-100 Å in length and 10-20 Å in diameter. While the eukaryotic tunnels are, on average, shorter and substantially narrower than prokaryote ones1.\nIn all domains of life, the tunnel features a universally conserved narrow region downstream of the PTC, so-called constriction site. However, the eukaryotic exit tunnel exhibit an additional (second) constriction site due to the modified structure of the surrounding ribosomal proteins.\n\n\n\nIllustration of the tunnel structure of H.sapiens."
  },
  {
    "objectID": "posts/sy mds tunnel/index.html#ribosome-dataset",
    "href": "posts/sy mds tunnel/index.html#ribosome-dataset",
    "title": "Multi Dimensional Scaling of ribosome exit tunnel shapes",
    "section": "Ribosome Dataset",
    "text": "Ribosome Dataset\nCryo-EM reconstructions and X-ray crystallography structures of ribosomes were retrived from the Protein Data Bank (https://www.rcsb.org) including 762 structures across 34 species domain.\nThe exit tunnels were extracted from the ribosomes using our developed tunnel-searching pipeline based on the MOLE cavity extraction algorithm developed by Sehnal et al.2."
  },
  {
    "objectID": "posts/sy mds tunnel/index.html#pairwise-distance",
    "href": "posts/sy mds tunnel/index.html#pairwise-distance",
    "title": "Multi Dimensional Scaling of ribosome exit tunnel shapes",
    "section": "Pairwise Distance",
    "text": "Pairwise Distance\nTo simplify the geomertic comparisons, we first reduced the tunnel structure into a coordinate set that describes both the centerline trajectory and the tunnel radius at each centerline position,\nWe then applied the pairwise distance metrics developed by Dao Duc et al.1 to compute the geometric similarity between tunnels. More details can be found in the previous work1.\n\n\n\nPairwise comparison of radial varaition plots between H.sapiens and E.coli"
  },
  {
    "objectID": "posts/sy mds tunnel/index.html#mds",
    "href": "posts/sy mds tunnel/index.html#mds",
    "title": "Multi Dimensional Scaling of ribosome exit tunnel shapes",
    "section": "MDS",
    "text": "MDS\nThe Multidimensional Scaling (MDS) method developed by Li et al.3 was applied on the pairwise distance matrix to visualize the geometric similarity of tunnels. Each data point represents a single tunnel structure, and the Euclidean distance between data points represents the similarity.\n\n\n\nMDS plot of tunnel structures across prokaryotes and eukaryotes"
  },
  {
    "objectID": "posts/ribosome-tunnel-new/index.html",
    "href": "posts/ribosome-tunnel-new/index.html",
    "title": "3D tessellation of biomolecular cavities (new)",
    "section": "",
    "text": "We present a protocol to extract the surface of a biomolecular cavity for shape analysis and molecular simulations.\nWe apply and illustrate the protocol on the ribosome structure, which contains a subcompartment known as the ribosome exit tunnel. More details on the tunnel features and biological importance can be found in our previous works1,2. The protocol was also design to refine the output obtained from MOLE software3\n\n\n\nIllustration of the ribosome exit tunnel (from Dao Duc et al., NAR 2019)"
  },
  {
    "objectID": "posts/ribosome-tunnel-new/index.html#summary-and-background",
    "href": "posts/ribosome-tunnel-new/index.html#summary-and-background",
    "title": "3D tessellation of biomolecular cavities (new)",
    "section": "",
    "text": "We present a protocol to extract the surface of a biomolecular cavity for shape analysis and molecular simulations.\nWe apply and illustrate the protocol on the ribosome structure, which contains a subcompartment known as the ribosome exit tunnel. More details on the tunnel features and biological importance can be found in our previous works1,2. The protocol was also design to refine the output obtained from MOLE software3\n\n\n\nIllustration of the ribosome exit tunnel (from Dao Duc et al., NAR 2019)"
  },
  {
    "objectID": "posts/ribosome-tunnel-new/index.html#data-preparation",
    "href": "posts/ribosome-tunnel-new/index.html#data-preparation",
    "title": "3D tessellation of biomolecular cavities (new)",
    "section": "0. Data preparation",
    "text": "0. Data preparation"
  },
  {
    "objectID": "posts/ribosome-tunnel-new/index.html#pre-processing",
    "href": "posts/ribosome-tunnel-new/index.html#pre-processing",
    "title": "3D tessellation of biomolecular cavities (new)",
    "section": "1. Pre-processing",
    "text": "1. Pre-processing"
  },
  {
    "objectID": "posts/ribosome-tunnel-new/index.html#voxelization",
    "href": "posts/ribosome-tunnel-new/index.html#voxelization",
    "title": "3D tessellation of biomolecular cavities (new)",
    "section": "2. Voxelization",
    "text": "2. Voxelization"
  },
  {
    "objectID": "posts/ribosome-tunnel-new/index.html#tessellation",
    "href": "posts/ribosome-tunnel-new/index.html#tessellation",
    "title": "3D tessellation of biomolecular cavities (new)",
    "section": "3. Tessellation",
    "text": "3. Tessellation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biological shape analysis (under construction)",
    "section": "",
    "text": "Multi Dimensional Scaling of ribosome exit tunnel shapes\n\n\nAnalyze and compare the geometry of the ribosome exit tunnel\n\n\n\ncryo-EM\n\n\nribosome\n\n\nMDS\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nShiqi Yu, Artem Kushner, Khanh Dao Duc\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Eye Tracking Data\n\n\n\n\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nLisa\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha Shapes in 2D and 3D\n\n\n\n\n\n\ntheory\n\n\n\n\n\n\n\n\n\nAug 14, 2024\n\n\nWenjun Zhao\n\n\n\n\n\n\n\n\n\n\n\n\nHand gesture classification with EMG data using Riemannian metrics\n\n\n\n\n\n\ntheory\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nGeomstats\n\n\n\n\n\n\n\n\n\n\n\n\nQuasiconformal mapping in 1D\n\n\n\n\n\n\ntheory\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nClément Soubrier\n\n\n\n\n\n\n\n\n\n\n\n\n3D tessellation of biomolecular cavities (new)\n\n\nProtocol for analyzing the ribosome exit tunnel\n\n\n\nexample\n\n\ncryo-EM\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\nArtem Kushner, Khanh Dao Duc\n\n\n\n\n\n\n\n\n\n\n\n\nPage creation (MATH 612)\n\n\nInstructions and tips for MATH 612 students\n\n\n\nMATH 612\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting cell geometry from AFM\n\n\nPart 1: Static analysis\n\n\n\nbiology\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nClément Soubrier, Khanh Dao Duc\n\n\n\n\n\n\n\n\n\n\n\n\nElastic metric for cell shape analysis\n\n\n\n\n\n\nbiology\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nClément Soubrier, Khanh Dao Duc\n\n\n\n\n\n\n\n\n\n\n\n\nAlignment of 3D volumes with Optimal Transport\n\n\nApplication to cryoEM density maps\n\n\n\nbiology\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nClément Soubrier, Khanh Dao Duc\n\n\n\n\n\n\n\n\n\n\n\n\nPoint cloud representation of 3D volumes\n\n\nApplication to cryoEM density maps\n\n\n\nbiology\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nClément Soubrier, Khanh Dao Duc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MATH-612/index.html#instructions",
    "href": "posts/MATH-612/index.html#instructions",
    "title": "Page creation (MATH 612)",
    "section": "Instructions",
    "text": "Instructions\n\nFollow all guidelines for assignments and projects.\nSubmit your work through the designated platforms by the due dates.\nCollaborate with peers but ensure individual work is original."
  },
  {
    "objectID": "posts/MATH-612/index.html#tips",
    "href": "posts/MATH-612/index.html#tips",
    "title": "Page creation (MATH 612)",
    "section": "Tips",
    "text": "Tips\n\nJupyter: Use Jupyter Notebooks for interactive coding and documentation. Great for running small code snippets and visualizing data. Learn more in the Jupyter Notebook Documentation.\nVS Code: A powerful IDE for writing and debugging code. Download it here, and install relevant extensions for Python and LaTeX.\nEnvironments: Use virtual environments like venv or conda to manage dependencies and ensure consistent results across different setups.\nQuarto: Use Quarto for creating high-quality documents, reports, and presentations from your code. It supports markdown and integrates seamlessly with Jupyter and VS Code for reproducible analysis and publication. Check out the Quarto Guide for more information. To get started quickly, you can refer to this GitHub Repository."
  },
  {
    "objectID": "posts/MATH-612/index.html#using-github",
    "href": "posts/MATH-612/index.html#using-github",
    "title": "Page creation (MATH 612)",
    "section": "Using GitHub",
    "text": "Using GitHub\n\nCreate a GitHub Account: Sign up at GitHub.com.\nRepositories: Start by creating a repository to host your project files. Learn how in GitHub’s guide to repositories. Use a .gitignore file to exclude unnecessary files.\nBranches: Work on separate branches (main, dev, feature branches) to manage different versions of your project. More details in GitHub’s guide on branching.\nMerges: Merge changes into the main branch only after thorough review and testing. Learn about merging branches.\nCommit Messages: Write clear, descriptive commit messages to document changes effectively. Follow the best practices for commit messages."
  },
  {
    "objectID": "posts/MATH-612/index.html#using-quarto-to-create-blog-posts",
    "href": "posts/MATH-612/index.html#using-quarto-to-create-blog-posts",
    "title": "Page creation (MATH 612)",
    "section": "Using Quarto to create blog posts",
    "text": "Using Quarto to create blog posts\n\nLog into GitHub: Make sure you have an account and are logged in.\nSend your account username/email to kdd@math.ubc.ca: This is needed to be added to the organization.\nClone the repository: After being added to the organization, clone the repository: https://github.com/bioshape-analysis/blog.\ngit clone https://github.com/bioshape-analysis/blog`\nCreate a new branch: To contribute to the blog, create a new branch using:\ngit checkout -b &lt;branch_name&gt;\n\nVerify your branch and repository location: Use the following command to check if you are in the correct branch and repository:\ngit status\nThis command will show you the current branch you are on and the status of your working directory, ensuring you are working in the right place\n\nNavigate to posts: Go into the posts directory (found here). Create a new folder with a name that represents the content of your blog post.\nCreate or upload your content:\n\nIf using Jupyter Notebooks, upload your .ipynb file.\nIf preferred, create a new notebook for your post. Once done, convert it into Quarto using the command:\nquarto convert your_jupyter_notebook.ipynb -o output_file.qmd\n\nEdit the YAML in your .qmd file: Ensure your YAML is consistent with the main template. For example:\n---\ntitle: \"Title of your blog post\"\ndate: \"Date\" # Format example: August 9 2024\nauthor:\n  - name: \"Your Name\" \ncategories: [] # [biology, bioinformatics, theory, etc.]\nbibliography: references.bib # If referencing anything\n---\nFeel free to add further formatting, but ensure it remains consistent with the main template.\nDelete your Jupyter notebook: After converting it to a .qmd file, delete the original .ipynb file to prevent duplication in the blog post.\nCommit and push your changes: After completing your .qmd file, push your branch to GitHub. A pull request will be automatically created, and once reviewed, it will be merged into the main branch.\n\n\nAdditional Information for Quarto:\n\nAdd Images: You can add images to your Quarto document using markdown syntax:\n![Image Description](path/to/image.png)\nTo add images from a URL:\n![Image Description](https://example.com/image.png)\nAdd References: Manage references by creating a bibliography.bib file with your references in BibTeX format. Link the bibliography file in your Quarto document header (YAML). Cite references in your text using the following syntax:\nThis is a citation [@citation_key].\nOther Edits: Add headers, footnotes, and other markdown features as needed. Customize the layout by editing the YAML header."
  },
  {
    "objectID": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html",
    "href": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html",
    "title": "Hand gesture classification with EMG data using Riemannian metrics",
    "section": "",
    "text": "Geomstats is a Python package for computations, statistics, machine learning, and deep learning on manifolds (Miolane et al. 2020).\nThe package is organized into two main modules: geometry and learning. The module geometry implements differential geometry: manifolds, Lie groups, fiber bundles, shape spaces, information manifolds, Riemannian metrics, and more. The module learning implements statistics and learning algorithms for data on manifolds. Users can choose between backends: NumPy, Autograd, or PyTorch.\nThis notebook is adapted from here\nIn this notebook we are using EMG time series collected by 8 electrodes placed on the arm skin. We are going to show how to:\n\nProcess these kind of signal into covariance matrices that we can manipulate with geomstats tools.\nHow to apply ML algorithms on this data to classify four different hand gestures present in the data (Rock, Paper, Scissors, Ok).\nHow do the different methods (using Riemanian metrics, projecting on tangent space, Euclidean metric) compare to each other."
  },
  {
    "objectID": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#introduction",
    "href": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#introduction",
    "title": "Hand gesture classification with EMG data using Riemannian metrics",
    "section": "",
    "text": "Geomstats is a Python package for computations, statistics, machine learning, and deep learning on manifolds (Miolane et al. 2020).\nThe package is organized into two main modules: geometry and learning. The module geometry implements differential geometry: manifolds, Lie groups, fiber bundles, shape spaces, information manifolds, Riemannian metrics, and more. The module learning implements statistics and learning algorithms for data on manifolds. Users can choose between backends: NumPy, Autograd, or PyTorch.\nThis notebook is adapted from here\nIn this notebook we are using EMG time series collected by 8 electrodes placed on the arm skin. We are going to show how to:\n\nProcess these kind of signal into covariance matrices that we can manipulate with geomstats tools.\nHow to apply ML algorithms on this data to classify four different hand gestures present in the data (Rock, Paper, Scissors, Ok).\nHow do the different methods (using Riemanian metrics, projecting on tangent space, Euclidean metric) compare to each other."
  },
  {
    "objectID": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#context",
    "href": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#context",
    "title": "Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Context",
    "text": "Context\nThe data are acquired from somOS-interface: an sEMG armband that allows you to interact via bluetooth with an Android smartphone (you can contact Marius Guerard (marius.guerard@gmail.com) or Renaud Renault (renaud.armand.renault@gmail.com) for more info on how to make this kind of armband yourself).\nAn example of application is to record static signs that are linked with different actions (moving a cursor and clicking, sign recognition for command based personal assistants, …). In these experiments, we want to evaluate the difference in performance (measured as the accuracy of sign recognition) between three different real life situations where we change the conditions of training (when user record signs or “calibrate” the device) and testing (when the app guess what sign the user is doing):\n\n\nWhat is the accuracy when doing sign recognition right after training?\n\n\nWhat is the accuracy when calibrating, removing and replacing the armband at the same position and then testing?\n\n\nWhat is the accuracy when calibrating, removing the armband and giving it to someone else that is testing it without calibration?\n\n\nTo simulate these situations, we record data from two different users (rr and mg) and in two different sessions (s1 or s2). The user put the bracelet before every session and remove it after every session.\nQuick description of the data:\n\nEach row corresponds to one acquisition, there is an acquisition every ~4 ms for 8 electrodes which correspond to a 250Hz acquisition rate.\nThe time column is in ms.\nThe columns c0 to c7 correspond to the electrical value recorded at each of the 8 electrodes (arbitrary unit).\nThe label correspond to the sign being recorded by the user at this time point (‘rest’, ‘rock’, ‘paper’, ‘scissors’, or ‘ok). ’rest’ correspond to a rested arm.\nthe exp identify the user (rr and mg) and the session (s1 or s2)\n\nNote: Another interesting use case, not explored in this notebook, would be to test what is the accruacy when calibrating, removing the armband and giving it to someone else that is calibrating it on its own arm before testing it. The idea being that transfer learning might help getting better results (or faster calibration) than calibrating on one user.\n\n\nShow/Hide Code\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport geomstats.backend as gs\n\nmatplotlib.interactive(True)\ngs.random.seed(2021)\n\n\nINFO: Using numpy backend"
  },
  {
    "objectID": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#parameters",
    "href": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#parameters",
    "title": "Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Parameters",
    "text": "Parameters\n\n\nShow/Hide Code\nN_ELECTRODES = 8\nN_SIGNS = 4"
  },
  {
    "objectID": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#the-data",
    "href": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#the-data",
    "title": "Hand gesture classification with EMG data using Riemannian metrics",
    "section": "The Data",
    "text": "The Data\n\n\nShow/Hide Code\nimport geomstats.datasets.utils as data_utils\n\ndata = data_utils.load_emg()\n\n\n\n\nShow/Hide Code\ndata.head()\n\n\n\n\n\n\n\n\n\ntime\nc0\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nlabel\nexp\n\n\n\n\n0\n23\n127\n123\n128\n134\n125\n128\n130\n124\nrest\nmg_s1\n\n\n1\n28\n126\n130\n128\n119\n129\n128\n126\n133\nrest\nmg_s1\n\n\n2\n32\n129\n130\n127\n125\n129\n129\n127\n130\nrest\nmg_s1\n\n\n3\n36\n127\n128\n126\n123\n128\n127\n125\n131\nrest\nmg_s1\n\n\n4\n40\n127\n128\n129\n124\n127\n129\n127\n128\nrest\nmg_s1\n\n\n\n\n\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots(N_SIGNS, figsize=(10, 7))\nlabel_list = [\"rock\", \"scissors\", \"paper\", \"ok\"]\nfor i, label_i in enumerate(label_list):\n    sign_df = data[data.label == label_i].iloc[:100]\n    for electrode in range(N_ELECTRODES):\n        ax[i].plot(sign_df.iloc[:, 1 + electrode])\n        ax[i].title.set_text(label_i)\n\n\n\n\n\n\n\n\n\nWe are removing the sign ‘rest’ for the rest of the analysis.\n\n\nShow/Hide Code\ndata = data[data.label != \"rest\"]\n\n\n\nPreprocessing into covariance matrices\n\n\nShow/Hide Code\nimport numpy as np\n\n### Parameters.\nN_STEPS = 100\nLABEL_MAP = {\"rock\": 0, \"scissors\": 1, \"paper\": 2, \"ok\": 3}\nMARGIN = 1000\n\n\nUnpacking data into arrays for batching\n\n\nShow/Hide Code\ndata_dict = {\n    \"time\": gs.array(data.time),\n    \"raw_data\": gs.array(data[[\"c{}\".format(i) for i in range(N_ELECTRODES)]]),\n    \"label\": gs.array(data.label),\n    \"exp\": gs.array(data.exp),\n}\n\n\n\n\nShow/Hide Code\nfrom geomstats.datasets.prepare_emg_data import TimeSeriesCovariance\n\ncov_data = TimeSeriesCovariance(data_dict, N_STEPS, N_ELECTRODES, LABEL_MAP, MARGIN)\ncov_data.transform()\n\n\nWe check that these matrics belong to the space of SPD matrices.\n\n\nShow/Hide Code\nfrom geomstats.geometry.spd_matrices import SPDMatrices\n\nmanifold = SPDMatrices(N_ELECTRODES, equip=False)\n\n\n\n\nShow/Hide Code\ngs.all(manifold.belongs(cov_data.covs))\n\n\nTrue\n\n\n\nCovariances plot of the euclidean average\n\n\nShow/Hide Code\nfig, ax = plt.subplots(2, 2, figsize=(10, 7))\nfor label_i, i in cov_data.label_map.items():\n    label_ids = np.where(cov_data.labels == i)[0]\n    sign_cov_mat = cov_data.covs[label_ids]\n    mean_cov = np.mean(sign_cov_mat, axis=0)\n    ax[i // 2, i % 2].matshow(mean_cov)\n    ax[i // 2, i % 2].title.set_text(label_i)\n\n\n\n\n\n\n\n\n\nLooking at the euclidean average of the spd matrices for each sign, does not show a striking difference between 3 of our signs (scissors, paper, and ok). Minimum Distance to Mean (MDM) algorithm will probably performed poorly if using euclidean mean here.\n\n\nCovariances plot of the Frechet Mean of the affine invariant metric\n\n\nShow/Hide Code\nfrom geomstats.geometry.spd_matrices import SPDAffineMetric\nfrom geomstats.learning.frechet_mean import FrechetMean\n\n\n\n\nShow/Hide Code\nmanifold.equip_with_metric(SPDAffineMetric)\n\nmean_affine = FrechetMean(manifold)\n\n\n\n\nShow/Hide Code\nfig, ax = plt.subplots(2, 2, figsize=(10, 7))\nfor label_i, i in cov_data.label_map.items():\n    label_ids = np.where(cov_data.labels == i)[0]\n    sign_cov_mat = cov_data.covs[label_ids]\n    mean_affine.fit(X=sign_cov_mat)\n    mean_cov = mean_affine.estimate_\n    ax[i // 2, i % 2].matshow(mean_cov)\n    ax[i // 2, i % 2].title.set_text(label_i)\n\n\n\n\n\n\n\n\n\nWe see that the average matrices computed using the affine invariant metric are now more differenciated from each other and can potentially give better results, when using MDM to predict the sign linked to a matrix sample."
  },
  {
    "objectID": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#sign-classification",
    "href": "posts/convert-notebook-to-quarto/12_rwa_emg_sign_classification.html#sign-classification",
    "title": "Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Sign Classification",
    "text": "Sign Classification\nWe are now going to train some classifiers on those matrices to see how we can accurately discriminate these 4 hand positions. The baseline accuracy is defined as the accuracy we get by randomly guessing the signs. In our case, the baseline accuracy is 25%.\n\n\nShow/Hide Code\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nShow/Hide Code\n# Hiding the numerous sklearn warnings\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nShow/Hide Code\nimport tensorflow as tf\nfrom scikeras.wrappers import KerasClassifier\n\n\nN_EPOCHS is the number of epochs on which to train the MLP. Recommended is ~100\n\n\nShow/Hide Code\nN_EPOCHS = 10\nN_FEATURES = int(N_ELECTRODES * (N_ELECTRODES + 1) / 2)\n\n\n\nA. Test on the same session and user as Training/Calibration\nIn this first part we are training our model on the same session that we are testing it on. In real life, it corresponds to a user calibrating his armband right before using it. To do this, we are splitting every session in k-folds, training on \\((k-1)\\) fold to test on the \\(k^{th}\\) last fold.\n\n\nShow/Hide Code\nclass ExpResults:\n    \"\"\"Class handling the score collection and plotting among the different experiments.\"\"\"\n\n    def __init__(self, exps):\n        self.exps = exps\n        self.results = {}\n        self.exp_ids = {}\n        # Compute the index corresponding to each session only once at initialization.\n        for exp in set(self.exps):\n            self.exp_ids[exp] = np.where(self.exps == exp)[0]\n\n    def add_result(self, model_name, model, X, y):\n        \"\"\"Add the results from the cross validated pipeline.\n\n        For the model 'pipeline', it will add the cross validated results of every session in the model_name\n        entry of self.results.\n\n        Parameters\n        ----------\n        model_name : str\n            Name of the pipeline/model that we are adding results from.\n        model : sklearn.pipeline.Pipeline\n            sklearn pipeline that we are evaluating.\n        X : array\n            data that we are ingesting in the pipeline.\n        y : array\n            labels corresponding to the data.\n        \"\"\"\n        self.results[model_name] = {\n            \"fit_time\": [],\n            \"score_time\": [],\n            \"test_score\": [],\n            \"train_score\": [],\n        }\n        for exp in self.exp_ids.keys():\n            ids = self.exp_ids[exp]\n            exp_result = cross_validate(\n                pipeline, X[ids], y[ids], return_train_score=True\n            )\n            for key in exp_result.keys():\n                self.results[model_name][key] += list(exp_result[key])\n        print(\n            \"Average training score: {:.4f}, Average test score: {:.4f}\".format(\n                np.mean(self.results[model_name][\"train_score\"]),\n                np.mean(self.results[model_name][\"test_score\"]),\n            )\n        )\n\n    def plot_results(\n        self,\n        title,\n        variables,\n        err_bar=None,\n        save_name=None,\n        xlabel=\"Model\",\n        ylabel=\"Acc\",\n    ):\n        \"\"\"Plot bar plot comparing the different pipelines' results.\n\n        Compare the results added previously using the 'add_result' method with bar plots.\n\n        Parameters\n        ----------\n        title : str\n            Title of the plot.\n        variables : list of array\n            List of the variables to plot (e.g. train_score, test_score,...)\n        err_bar : list of float\n            list of error to use for plotting error bars. If None, std is used by default.\n        save_name : str\n            path to save the plot. If None, plot is not saved.\n        xlabel : str\n            Label of the x-axis.\n        ylabel : str\n            Label of the y-axis.\n        \"\"\"\n        ### Some defaults parameters.\n        w = 0.5\n        colors = [\"b\", \"r\", \"gray\"]\n\n        ### Reshaping the results for plotting.\n        x_labels = self.results.keys()\n        list_vec = []\n        for variable in variables:\n            list_vec.append(\n                np.array(\n                    [self.results[model][variable] for model in x_labels]\n                ).transpose()\n            )\n        rand_m1 = lambda size: np.random.random(size) * 2 - 1\n\n        ### Plots parameters.\n        label_loc = np.arange(len(x_labels))\n        center_bar = [w * (i - 0.5) for i in range(len(list_vec))]\n\n        ### Plots values.\n        avg_vec = [np.nanmean(vec, axis=0) for vec in list_vec]\n        if err_bar is None:\n            err_bar = [np.nanstd(vec, axis=0) for vec in list_vec]\n\n        ### Plotting the data.\n        fig, ax = plt.subplots(figsize=(10, 7))\n        for i, vec in enumerate(list_vec):\n            label_i = variable[i] + \" (n = {})\".format(len(vec))\n            ax.bar(\n                label_loc + center_bar[i],\n                avg_vec[i],\n                w,\n                label=label_i,\n                yerr=err_bar[i],\n                color=colors[i],\n                alpha=0.6,\n            )\n            for j, x in enumerate(label_loc):\n                ax.scatter(\n                    (x + center_bar[i]) + rand_m1(vec[:, j].size) * w / 4,\n                    vec[:, j],\n                    color=colors[i],\n                    edgecolor=\"k\",\n                )\n\n        # Add some text for labels, title and custom x-axis tick labels, etc.\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n        ax.set_title(title)\n        ax.set_xticks(label_loc)\n        ax.set_xticklabels(x_labels)\n        ax.legend()\n        plt.legend()\n\n        ### Saving the figure with a timestamp as a name.\n        if save_name is not None:\n            plt.savefig(save_name)\n\n\n\n\nShow/Hide Code\nexp_arr = data.exp.iloc[cov_data.batches]\nintra_sessions_results = ExpResults(exp_arr)\n\n\n\nA.0. Using Logistic Regression on the vectorized Matrix (Euclidean Method)\n\n\nShow/Hide Code\npipeline = Pipeline(\n    steps=[\n        (\"standardize\", StandardScaler()),\n        (\"logreg\", LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")),\n    ]\n)\n\nintra_sessions_results.add_result(\n    model_name=\"logreg_eucl\", model=pipeline, X=cov_data.covecs, y=cov_data.labels\n)\n\n\nAverage training score: 0.9938, Average test score: 0.9168\n\n\n\n\nA.1. Using MLP on the vectorized Matrix (Euclidean Method)\n\n\nShow/Hide Code\ndef create_model(weights=\"initial.weights.h5\", n_features=N_FEATURES, n_signs=N_SIGNS):\n    \"\"\"Create model.\n\n    Function to create model, required for using KerasClassifier and wrapp a Keras model inside a\n    scikitlearn form.\n    We added a weight saving/loading to remove the randomness of the weight initialization (for better comparison).\n    \"\"\"\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Dense(\n                n_features, activation=\"relu\", input_shape=(n_features,)\n            ),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(17, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(n_signs, activation=\"softmax\"),\n        ]\n    )\n\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=\"rmsprop\",\n        metrics=[\"accuracy\"],\n    )\n    if weights is None:\n        model.save_weights(\"initial.weights.h5\")\n    else:\n        model.load_weights(weights)\n    return model\n\n\ndef create_model_covariance(weights=\"initial.weights.h5\"):\n    \"\"\"Create model covariance.\"\"\"\n    return create_model(weights=weights, n_features=N_FEATURES)\n\n\nUse the line below to generate the ‘initial.weights.h5’ file\n\n\nShow/Hide Code\ngenerate_weights = create_model(weights=None)\n\n\n\n\nShow/Hide Code\npipeline = Pipeline(\n    steps=[\n        (\"standardize\", StandardScaler()),\n        (\"mlp\", KerasClassifier(build_fn=create_model, epochs=N_EPOCHS, verbose=0)),\n    ]\n)\n\nintra_sessions_results.add_result(\n    model_name=\"mlp_eucl\", model=pipeline, X=cov_data.covecs, y=cov_data.labels\n)\n\n\nAverage training score: 0.9598, Average test score: 0.8768\n\n\n\n\nA.2. Using Tangent space projection + Logistic Regression\n\n\nShow/Hide Code\nfrom geomstats.learning.preprocessing import ToTangentSpace\n\npipeline = Pipeline(\n    steps=[\n        (\"feature_ext\", ToTangentSpace(manifold)),\n        (\"standardize\", StandardScaler()),\n        (\"logreg\", LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")),\n    ]\n)\n\nintra_sessions_results.add_result(\n    model_name=\"logreg_affinvariant_tangent\",\n    model=pipeline,\n    X=cov_data.covs,\n    y=cov_data.labels,\n)\n\n\nAverage training score: 0.9959, Average test score: 0.9200\n\n\n\n\nA.3. Using Tangent space projection + MLP\n\n\nShow/Hide Code\npipeline = Pipeline(\n    steps=[\n        (\"feature_ext\", ToTangentSpace(manifold)),\n        (\"standardize\", StandardScaler()),\n        (\n            \"mlp\",\n            KerasClassifier(\n                build_fn=create_model_covariance, epochs=N_EPOCHS, verbose=0\n            ),\n        ),\n    ]\n)\n\nintra_sessions_results.add_result(\n    model_name=\"mlp_affinvariant_tangent\",\n    model=pipeline,\n    X=cov_data.covs,\n    y=cov_data.labels,\n)\n\n\nAverage training score: 0.9744, Average test score: 0.8963\n\n\n\n\nA.4. Using Euclidean MDM\n\n\nShow/Hide Code\nfrom geomstats.geometry.spd_matrices import SPDEuclideanMetric\nfrom geomstats.learning.mdm import RiemannianMinimumDistanceToMean\n\nmanifold.equip_with_metric(SPDEuclideanMetric)\n\npipeline = Pipeline(\n    steps=[\n        (\n            \"clf\",\n            RiemannianMinimumDistanceToMean(manifold),\n        )\n    ]\n)\n\nintra_sessions_results.add_result(\n    model_name=\"mdm_eucl\", model=pipeline, X=cov_data.covs, y=cov_data.labels\n)\n\n\nAverage training score: 0.8498, Average test score: 0.7999\n\n\n\n\nA.5. Using Riemannian MDM\n\n\nShow/Hide Code\npipeline = Pipeline(\n    steps=[\n        (\n            \"clf\",\n            RiemannianMinimumDistanceToMean(manifold),\n        )\n    ]\n)\n\nintra_sessions_results.add_result(\n    model_name=\"mdm_affinvariant\", model=pipeline, X=cov_data.covs, y=cov_data.labels\n)\n\n\nAverage training score: 0.8498, Average test score: 0.7999\n\n\n\n\nSummary plots\n\n\nShow/Hide Code\nintra_sessions_results.plot_results(\"intra_sess\", [\"test_score\"])"
  },
  {
    "objectID": "posts/ET/ey.html",
    "href": "posts/ET/ey.html",
    "title": "Analysis of Eye Tracking Data",
    "section": "",
    "text": "Eye Tracking\n\nEye tracking (ET) is a process by which a device measures the gaze of a participant – with a number of variables that can be captured, such as duration of fixation, re-fixation (go-backs), saccades, blinking, pupillary response. The ‘strong eye-mind hypothesis’ provides the theoretical ground where the underlying assumption is that duration of fixation is a reflection of preference, and that information is processed with immediacy. ET also is a non-invasive technique that has recently garnered attention in autism research as a method to elucidate or gather more information about the supposed central cognitive deficit (Flack-Ytter et al., 2013, Senju et al., 2009).\n\nExperimental set up\n\n22 youth (13-17) with high functioning autism and without autism will be recruited into this study.Students will be brought into a quiet room and asked to read a manga comic displayed on a monitor connected to the eye tracking device (Tobii pro eye tracker, provided by Professor Conati’s lab)"
  },
  {
    "objectID": "posts/ET/ey.html#eye-tracking-backagroud",
    "href": "posts/ET/ey.html#eye-tracking-backagroud",
    "title": "Analysis of Eye Tracking Data",
    "section": "",
    "text": "Eye Tracking\n\nEye tracking (ET) is a process by which a device measures the gaze of a participant – with a number of variables that can be captured, such as duration of fixation, re-fixation (go-backs), saccades, blinking, pupillary response. The ‘strong eye-mind hypothesis’ provides the theoretical ground where the underlying assumption is that duration of fixation is a reflection of preference, and that information is processed with immediacy. ET also is a non-invasive technique that has recently garnered attention in autism research as a method to elucidate or gather more information about the supposed central cognitive deficit (Flack-Ytter et al., 2013, Senju et al., 2009).\n\nExperimental set up\n\n22 youth (13-17) with high functioning autism and without autism will be recruited into this study.Students will be brought into a quiet room and asked to read a manga comic displayed on a monitor connected to the eye tracking device (Tobii pro eye tracker, provided by Professor Conati’s lab)"
  },
  {
    "objectID": "posts/ET/ey.html#visualisation",
    "href": "posts/ET/ey.html#visualisation",
    "title": "Analysis of Eye Tracking Data",
    "section": "2 Visualisation",
    "text": "2 Visualisation\nOne way of visualizing your data in Tobii Pro Lab is by creating Heat maps. Heat maps visualize where a participant’s (or a group of participants’) fixations or gaze data samples were distributed on a still image or a video frame. The distribution of the data is represented with colors.Each sample corresponds to a gaze point from the eye tracker, consistently sampled every 1.6 to 33 milliseconds (depending on the sampling data rate of the eye tracker). When using an I-VT Filter, it will group the raw eye tracking samples into fixations. The duration of each fixation depends on the gaze filter used to identify the fixations.\n\n\n\nHeatmap"
  },
  {
    "objectID": "posts/ET/ey.html#features",
    "href": "posts/ET/ey.html#features",
    "title": "Analysis of Eye Tracking Data",
    "section": "3 Features",
    "text": "3 Features\n\nData processing of eye tracking recordings\n\nTo run a statistical study on the data recorded, we carried out in two stages data processing. First using Tobio Pro Lab, then the EMADAT package. Following the experiments, the files are processed using Tobii Pro Lab software. We delimited the AOI for each page, manually pointed the gazes points for the 22 participants on the 12 selected pages. Then exported the data for each participant in a tsv format.\nThen EMDAT was used to generate the datasets. Indeed, to extract the gaze features we used EMDAT python 2.7. EMDAT stands for Eye Movement Data Analysis Toolkit, it is an open-source toolkit developed by our group. EMDAT receives three types of input folder: a folder containing the recordings from Tobii in a tsv format, a Segment folder containing the timestamp for the start and end of page reading for each participant, and an AOI folder containing the coordinates and the time spent per participant of each AOI per page. We have also automated the writing of the Segments and AOIs folders. Then we run the EMDAT script for each page. EMDAT also validates the quality of the recordings per page, here the parameter has been set to VALIDITY_METHOD = 1 (see documentation). In particular, we found that the quality of the data did not diminish over the course of the recordings.\n\nEye tracking features\n\nUpon following the data processing protocol, we extracted the following features:\n\nnumber of fixation (quantitative feature): The number of fixations denoted by is defined as the total number of fixations recorded over the total duration spent on a page by a participant.\nmean fixation duration (duration feature): The mean fixation duration denoted by is defined as as the average fixation duration during page reading.\nstandard deviation of the relative path angle (spatial feature): The standard deviation of the relative path angle denoted by is defined as as the average fixation duration during page reading.the standard deviation of the relative angle between two successive saccades. This component enables us to capture the consistency of a participant’s gaze pattern. The greater the standard deviation, the more likely the participant is to look across the different areas of a page."
  },
  {
    "objectID": "posts/ET/ey.html#t-test",
    "href": "posts/ET/ey.html#t-test",
    "title": "Analysis of Eye Tracking Data",
    "section": "4 T-test",
    "text": "4 T-test\nFirst, we wondered whether there were any major differences in the way the two groups read. To do this, we compared the two populations along the three axes - quantitative, duration and spatial - defined in the previous section. To quantify these differences, we used a t-test to compare the means of the distributions, and a Kolmogorov-Smirnov test to compare the distributions. Concerning the total number of fixations per page, the two populations seem to have the same characteristics (p-value&gt;0.1 and Cohen’s d=0.2) and to be from the same distribution (two sided K-s test p-value&gt;0.1). However, on the other two criteria, the autistic adolescents had a shorter mean fixation time and a lower standard deviation (p-value&lt;0.05, Cohen’s d &gt; 0.5), and their associated distribution was lower than that of the control population (less K-S test p-value&gt;0.1).\n\n\n\n\n\n\n\n\n\nT-test\nK-S test\n\n\n\n\nNum fixations\nNo statistically significant differences in the mean number of fixation (small effect size, two-sided p-value &gt; 0.1)\nThe distributions of the number of fixations per page look similar across the two populations (KS two-sided p-value &gt; 0.1)\n\n\nMean fixation duration\nND seems to have a shorter mean duration fixation (Negative medium effect size, two-sided p-value &lt; 0.01)\nThe ND mean fixation duration distribution is smaller than the NT mean fixation duration distribution (KS less p-value &gt; 0.1)\n\n\nStandard deviation relative path angle\nND seems to have on average a smaller std (Negative medium effect size, two-sided p-value &lt; 0.01)\nThe ND std relative path angle distribution is smaller than the NT std relative path angle distribution (KS less p-value &gt; 0.1)"
  }
]